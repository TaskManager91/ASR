{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "common_voice_train = load_dataset(\"common_voice\", \"de\", split=\"train\", cache_dir=\"D:\\Master\\wsl\\data\")\n",
    "common_voice_validation = load_dataset(\"common_voice\", \"de\", split=\"validation\", cache_dir=\"D:\\Master\\wsl\\data\")\n",
    "common_voice_test = load_dataset(\"common_voice\", \"de\", split=\"test\", cache_dir=\"D:\\Master\\wsl\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_validation = common_voice_validation.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_keep = '[^A-Za-zäüöß ]+'\n",
    "\n",
    "def remove_special_characters_chris(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_keep, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters_chris)\n",
    "common_voice_validation = common_voice_validation.map(remove_special_characters_chris)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters_chris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_validation.save_to_disk(\"E:/Master/data/val_text\")\n",
    "common_voice_train.save_to_disk(\"E:/Master/data/train_text\")\n",
    "common_voice_test.save_to_disk(\"E:/Master/data/test_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "train_text_dataset = load_from_disk(\"E:/Master/data/0_text/train_text\")\n",
    "val_text_dataset = load_from_disk(\"E:/Master/data/0_text/val_text\")\n",
    "test_text_dataset = load_from_disk(\"E:/Master/data/0_text/test_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy[cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf\n",
    "!python -m spacy download de_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy[transformers,cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"de_dep_news_trf\")\n",
    "nlp.max_length = 17000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_text = \"\"\n",
    "val_complete_text = \"\"\n",
    "test_complete_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246525\n",
      "15588\n",
      "15588\n"
     ]
    }
   ],
   "source": [
    "print(train_text_dataset.shape[0])\n",
    "print(val_text_dataset.shape[0])\n",
    "print(test_text_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Datasets to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 246525/246525 [00:13<00:00, 18018.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 15588/15588 [00:00<00:00, 32338.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 15588/15588 [00:00<00:00, 32679.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(train_text_dataset.shape[0])):\n",
    "    train_complete_text = train_complete_text + train_text_dataset[i]['sentence']\n",
    "\n",
    "for i in tqdm(range(val_text_dataset.shape[0])):\n",
    "    val_complete_text = val_complete_text + val_text_dataset[i]['sentence']\n",
    "\n",
    "for i in tqdm(range(test_text_dataset.shape[0])):\n",
    "    test_complete_text = test_complete_text + test_text_dataset[i]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16656802\n",
      "997429\n",
      "976676\n"
     ]
    }
   ],
   "source": [
    "print(len(train_complete_text))\n",
    "print(len(val_complete_text))\n",
    "print(len(test_complete_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Datasets with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process train @  2021-05-19 11:37:49.702686\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 18128999424 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f46df1e90710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start process train @ \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_doc_trf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_complete_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    998\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE109\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1000\u001b[1;33m                 \u001b[0merror_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1001\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mraise_error\u001b[1;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1503\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    993\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m                 \u001b[1;31m# This typically happens if a component is not initialized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy_transformers\\pipeline_component.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m    183\u001b[0m         \u001b[0minstall_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy_transformers\\pipeline_component.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFullTransformerBatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformerListener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlistener\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \"\"\"\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy_transformers\\layers\\transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, docs, is_train)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mwordpieces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     )\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordpieces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"logger\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mlog_gpu_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"logger\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"after forward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    287\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\layers\\pytorchwrapper.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mXtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mYtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch_backprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_dYtorch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtorch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\shims\\pytorch.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, is_train)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mArgsKwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\thinc\\shims\\pytorch.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"absolute\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 18128999424 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process train @ \", now)\n",
    "\n",
    "train_doc_trf = nlp(train_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process train @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_doc_sm.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_doc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process val @  2021-05-18 19:37:43.615021\n",
      "End process val @  2021-05-18 19:42:20.076160\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process val @ \", now)\n",
    "\n",
    "val_doc = nlp(val_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process val @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('val_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_doc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with open('val_doc_trf.pickle', 'rb') as handle:\n",
    "    val_doc_trf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process test @  2021-05-18 14:42:02.360915\n",
      "End process test @  2021-05-18 14:42:08.361938\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"start process test @ \", now)\n",
    "\n",
    "test_doc = nlp(test_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process test @ \", now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tokens that arent stop words or punctuations\n",
    "train_all = [token.text\n",
    "         for token in train_doc\n",
    "         if not token.is_punct]\n",
    "\n",
    "# all tokens that arent stop words or punctuations\n",
    "train_words = [token.text\n",
    "         for token in train_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "train_nouns = [token.text\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n"
     ]
    }
   ],
   "source": [
    "for word in train_words:\n",
    "    if(word == \"ausgebeutet\"):\n",
    "        print(\"found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train all:  2350690\n",
      "train words dict:  168237\n",
      "train words:  1138469\n",
      "train words dict:  167707\n",
      "train nouns:  143335\n",
      "train nouns dict:  26890\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"train all: \", len(train_all))\n",
    "train_all_freq = Counter(train_all)\n",
    "print(\"train words dict: \", len(train_all_freq))\n",
    "\n",
    "print(\"train words: \", len(train_words))\n",
    "train_words_freq = Counter(train_words)\n",
    "print(\"train words dict: \", len(train_words_freq))\n",
    "\n",
    "print(\"train nouns: \", len(train_nouns))\n",
    "train_nouns_freq = Counter(train_nouns)\n",
    "print(\"train nouns dict: \", len(train_nouns_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma nouns test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun tokens that arent stop words or punctuations\n",
    "train_nouns_lemma = [token.lemma_\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma Words: 143335\n",
      "Lemma freq Words 26758\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemma Noun Words:\", len(train_nouns_lemma))\n",
    "\n",
    "train_nouns_lemma_freq = Counter(train_nouns_lemma)\n",
    "\n",
    "print(\"Lemma Noun freq Words\", len(train_nouns_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare Lemma words 1 :  17082\n",
      "Number of rare Lemma words 2 :  20678\n",
      "Number of rare Lemma words 3 :  22212\n",
      "Number of rare Lemma words 4 :  23109\n",
      "Number of rare Lemma words 5 :  23670\n",
      "Number of rare Lemma words 6 :  24042\n",
      "Number of rare Lemma words 7 :  24333\n",
      "Number of rare Lemma words 8 :  24578\n",
      "Number of rare Lemma words 9 :  24782\n",
      "Number of rare Lemma words 10 :  24939\n"
     ]
    }
   ],
   "source": [
    "train_nouns_lemma_freq_keys = train_nouns_lemma_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(1,n):\n",
    "    \n",
    "    rare_lemma_words = []\n",
    "    for word in train_nouns_lemma_freq_keys:\n",
    "        \n",
    "        if train_nouns_lemma_freq[word] <= i:\n",
    "            rare_lemma_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare Lemma words \" + str(i) + \" : \", len(rare_lemma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['schiebers', 'sturmes', 'piscina', 'tankdeckels', 'fertigungsanlage', 'kopie', 'tauernfensters', 'staatsuniversität', 'modellautos', 'hufen', 'rennleitung', 'schutzumschlag', 'highschoolzeit', 'kontrabassisten', 'homiletik', 'heiliggeistordens', 'beifall', 'geschichtspolitik', 'instandsetzungstruppe', 'merowingern', 'austreiben', 'sesklokultur', 'ovale', 'trajektorie', 'umsatzbeteiligung', 'gesellschafterversammlung', 'wood', 'trialog', 'granada', 'bildpunkt', 'einsatzfall', 'zarismus', 'feudalismus', 'realschulempfehlung', 'boxsack', 'versuchsanordnung', 'christin', 'xte', 'matheprüfung', 'weihnachtsbaumdekoration', 'modelltheorie', 'böhme', 'unterkieferknochen', 'ormandy', 'cholerakonferenz', 'arbeitsbewertung', 'handymusik', 'komponist', 'audiodaten', 'barchfeld', 'antonio', 'adour', 'humorvoll', 'master', 'freileitung', 'gesundheitsschutzniveau', 'ausgabenprogramm', 'schiffsunglück', 'friedenszeiten', 'mennonitengemeinde', 'leitungsfunktion', 'tritt', 'sauna', 'stiftungsprofessur', 'fernheizwerk', 'mikrofossilien', 'altenhof', 'adsorption', 'karmels', 'lebensgewohnheiten', 'passunion', 'sitzheizung', 'lombardei', 'debüts', 'südsüdhandels', 'preistheorie', 'gebäuderückseite', 'beobachter', 'lagersystem', 'babyklappe', 'felsenlandjugendherberge', 'jugendherbergswerks', 'klopapier', 'zerrung', 'fünfinfünfauspuffanlage', 'geräuschentwicklung', 'hammerschlag', 'beschleunigen', 'landtagsausschuss', 'taiwaner', 'bruchsteinsockel', 'gehhilfe', 'kreisversammlung', 'städtchens', 'lehrbuch']\n"
     ]
    }
   ],
   "source": [
    "train_rare_lemma_words = []\n",
    "for word in train_nouns_lemma_freq_keys:\n",
    "    if train_nouns_lemma_freq[word] <= 1:\n",
    "        train_rare_lemma_words.append(word)\n",
    "\n",
    "print(train_rare_lemma_words[5:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemma words test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma Words: 1138469\n",
      "Lemma freq Words 151687\n"
     ]
    }
   ],
   "source": [
    "train_words_lemma = [token.lemma_\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct)]\n",
    "\n",
    "print(\"Lemma Words:\", len(train_words_lemma))\n",
    "train_words_lemma_freq = Counter(train_words_lemma)\n",
    "print(\"Lemma freq Words\", len(train_words_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "found!\n",
      "found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in train_words:\n",
    "    if word == \"rutschte\":\n",
    "        print(\"found!\")\n",
    "\n",
    "train_words_freq['rutschte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare Lemma words 1 :  84290\n",
      "Number of rare Lemma words 2 :  108559\n",
      "Number of rare Lemma words 3 :  119610\n",
      "Number of rare Lemma words 4 :  125815\n",
      "Number of rare Lemma words 5 :  129823\n",
      "Number of rare Lemma words 6 :  132562\n",
      "Number of rare Lemma words 7 :  134655\n",
      "Number of rare Lemma words 8 :  136284\n",
      "Number of rare Lemma words 9 :  137614\n",
      "Number of rare Lemma words 10 :  138702\n"
     ]
    }
   ],
   "source": [
    "train_words_lemma_freq_keys = train_words_lemma_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(1,n):\n",
    "    \n",
    "    train_rare_words = []\n",
    "    for word in train_words_lemma_freq_keys:\n",
    "        \n",
    "        if train_words_lemma_freq[word] <= i:\n",
    "            train_rare_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare Lemma words \" + str(i) + \" : \", len(train_rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alfried', 'abrakadabra', 'abdrücken', 'jadebusen', 'rohstoffknappheit', 'privatkraftwagen', 'indoarischen', 'schiebers', 'kartenleser', 'juniorengrandprixfinale', 'platzanweiser', 'sturmes', 'abkauft', 'fehlerkorrekturen', 'franzoseneinfall', 'knollig', 'obie', 'maissorten', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoffer', 'stahlfelgen', 'rutschen', 'tankdeckels', 'gerst', 'bestreicht', 'brötchenhälfte', 'ausgebeuteten', 'ausbeuter', 'mörderisch', 'verhandlungssache', 'luftkraftstoffgemisch', 'fertigungsanlage', 'emile', 'gomer', 'nassauidenstein', 'logoireihe', 'hereinholen', 'dünenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'schiefers', 'tauernfensters', 'gerichtsakten', 'ostwestfalens', 'funston', 'bambergs', 'czapski', 'staatsuniversität', 'vallaster', 'gifsuryvette', 'tuning', 'knolls', 'nauroth', 'zeitungswissenschaft', 'dietenhofen', 'armenisch', 'kunstgewerbemuseums', 'militärwaffen', 'lgötzen', 'kinderdentist', 'hilfst', 'zeigegeste', 'korrektes', 'dervi', 'erolu', 'monogame', 'voglers', 'handelsgeschäfte', 'gefäßformen', 'böhlau', 'quellenfrei', 'ipadressen', 'gnesiolutheraner', 'philippisten', 'zugerechnete', 'lakonischen', 'herzjesuverehrung', 'rennleitung', 'schutzumschlag', 'libris', 'savoiamarchetti', 'südatlantikluftverkehr', 'saintimier', 'pquier', 'gameplays', 'alkoholikers', 'anlegestellen', 'doktorexamen', 'verschaffelt', 'kristallographie']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] <= 1:\n",
    "        train_rare_words.append(word)\n",
    "\n",
    "print(train_rare_words[5:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare words 0 :  0\n",
      "Number of rare words 1 :  91121\n",
      "Number of rare words 2 :  118265\n",
      "Number of rare words 3 :  130694\n",
      "Number of rare words 4 :  137805\n",
      "Number of rare words 5 :  142409\n",
      "Number of rare words 6 :  145643\n",
      "Number of rare words 7 :  148076\n",
      "Number of rare words 8 :  149990\n",
      "Number of rare words 9 :  151534\n",
      "Number of rare words 10 :  152818\n"
     ]
    }
   ],
   "source": [
    "train_words_freq_keys = train_words_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(0,n):\n",
    "    \n",
    "    rare_words = []\n",
    "    for word in train_words_freq_keys:\n",
    "        \n",
    "        if train_words_freq[word] <= i:\n",
    "            rare_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare words \" + str(i) + \" : \", len(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91121\n",
      "['drannehmen', 'marktverständnis', 'heilbäder', 'bildbearbeitungsprogramme', 'muttersohnkonflikt', 'alfried', 'abrakadabra', 'abdrücken', 'jadebusen', 'rohstoffknappheit', 'privatkraftwagen', 'indoarischen', 'schiebers', 'kartenleser', 'juniorengrandprixfinale', 'platzanweiser', 'sturmes', 'abkauft', 'fehlerkorrekturen', 'franzoseneinfall', 'knollig', 'obie', 'maissorten', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoffer', 'stahlfelgen', 'rutschen', 'tankdeckels', 'gerst', 'bestreicht', 'brötchenhälfte', 'ausgebeuteten', 'ausbeuter', 'mörderisch', 'verhandlungssache', 'luftkraftstoffgemisch', 'fertigungsanlage', 'emile', 'gomer', 'nassauidenstein', 'logoireihe', 'hereinholen', 'dünenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'schiefers', 'tauernfensters', 'gerichtsakten', 'ostwestfalens', 'funston', 'bambergs', 'czapski', 'staatsuniversität', 'vallaster', 'gifsuryvette', 'tuning', 'knolls', 'nauroth', 'zeitungswissenschaft', 'dietenhofen', 'armenisch', 'kunstgewerbemuseums', 'militärwaffen', 'lgötzen', 'kinderdentist', 'hilfst', 'zeigegeste', 'korrektes', 'dervi', 'erolu', 'monogame', 'voglers', 'handelsgeschäfte', 'gefäßformen', 'böhlau', 'quellenfrei', 'ipadressen', 'gnesiolutheraner', 'philippisten', 'zugerechnete', 'lakonischen', 'herzjesuverehrung', 'rennleitung', 'schutzumschlag', 'libris', 'savoiamarchetti', 'südatlantikluftverkehr', 'saintimier', 'pquier', 'gameplays', 'alkoholikers', 'anlegestellen', 'doktorexamen', 'verschaffelt', 'kristallographie']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] <= 1:\n",
    "        train_rare_words.append(word)\n",
    "        \n",
    "print(len(train_rare_words))\n",
    "print(train_rare_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27144\n",
      "['romakindern', 'gemeinschaftsmitteln', 'erzwungener', 'qibla', 'minarette', 'wohngegenden', 'düsseldorfs', 'feldzeichen', 'humperdincks', 'feldherren', 'saratoga', 'startrekgeschichte', 'ziggy', 'domgemeinde', 'cell', 'verriss', 'reichranicki', 'hormonhaushalt', 'ogden', 'empfohlene', 'dustin', 'janka', 'unermesslich', 'kyrillischen', 'leberwurst', 'einzahlt', 'verschlingen', 'treffe', 'kettenhemd', 'zündkerze', 'aerts', 'radrennfahrers', 'modulare', 'kimbern', 'atomubooten', 'konfliktfall', 'bermel', 'convergence', 'triskele', 'badearzt', 'separatismus', 'abschlüssen', 'absurdes', 'frischluft', 'mittelasien', 'subg', 'großgruppe', 'schutzbereich', 'belvedere', 'ungedruckt', 'badoglio', 'harare', 'modellautos', 'selbstgespräche', 'oberwürzbach', 'niederwürzbach', 'kunstlehrer', 'südwestrundfunk', 'gemischtes', 'stereozentrum', 'plutos', 'rillen', 'promis', 'entwarnung', 'teilprobleme', 'ahoi', 'landratten', 'halbton', 'winkelmann', 'flecke', 'velum', 'brill', 'schattendasein', 'kindererziehung', 'luftwaffendivision', 'künstlervereins', 'weinmann', 'segelschulschiff', 'semaphor', 'sharpe', 'störungslinie', 'korntal', 'monarchistisch', 'traditionspflege', 'führungspersonal', 'kabale', 'figurativ', 'terrakotta', 'entwaffneten', 'bemis', 'worlds', 'optionalen', 'implantaten', 'leistungsschalter', 'hämoglobin', 'gipfelregion', 'jahnhalle', 'boal', 'porphyrine', 'absalon']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words_2 = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] == 2:\n",
    "        train_rare_words_2.append(word)\n",
    "        \n",
    "print(len(train_rare_words_2))\n",
    "print(train_rare_words_2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12429\n",
      "['wüstenbildung', 'totales', 'markanter', 'polarstern', 'kuppeln', 'landesmuseums', 'steinbock', 'moyne', 'leitthema', 'wechselhaft', 'kater', 'itzehoe', 'kriegsteilnehmer', 'artilleriebrigade', 'falschgeld', 'dry', 'analytischer', 'karasek', 'tabs', 'nässe', 'reifendruck', 'sachse', 'imperiums', 'tabea', 'keltisches', 'schmiedeberg', 'verursache', 'rechtwinkligen', 'untergattung', 'judenburg', 'swahili', 'grundstoff', 'straßenverkehrsordnung', 'perfektes', 'lillehammer', 'kompost', 'hacke', 'chiraler', 'quarterback', 'alkoholproblem', 'synthetisieren', 'echternach', 'tampico', 'unterernährung', 'portiert', 'militärgericht', 'polemik', 'aphorismen', 'synchronisieren', 'truppenkommando', 'gleitet', 'doppelphaeton', 'subregens', 'herold', 'taian', 'schollen', 'erinnerungskultur', 'lauenburg', 'wurfarm', 'verbal', 'dankend', 'nervenzusammenbruch', 'quintus', 'taktiker', 'umweltpolitische', 'kernreaktor', 'geometrien', 'arbeitsgang', 'eskalierte', 'maschinengewehren', 'sachverständige', 'helge', 'schnarcht', 'bear', 'ghettos', 'stabhochsprung', 'brahma', 'asten', 'profilieren', 'ephremidis', 'verdursten', 'archivaufnahmen', 'sumatra', 'bildpunkt', 'zahnstocher', 'flugsicherung', 'wetterdienst', 'einsatzleiter', 'einsatzfall', 'diffusor', 'zentralisierte', 'zenker', 'kommentator', 'kosmetika', 'zwiespältigen', 'sünden', 'schwachsinn', 'bergangsphase', 'zugspitze', 'burley']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words_3 = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] == 3:\n",
    "        train_rare_words_3.append(word)\n",
    "        \n",
    "print(len(train_rare_words_3))\n",
    "print(train_rare_words_3[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7111\n",
      "['haushaltslage', 'konfliktparteien', 'oberflächenwasser', 'teuersten', 'fliegende', 'rückten', 'gulasch', 'marmelade', 'fraktionslos', 'mittelalterlich', 'friedliebenden', 'irvine', 'personalunion', 'biomüll', 'entsorgen', 'safran', 'gel', 'weserbergland', 'ex', 'schwindelfreiheit', 'reichtums', 'hauptroute', 'königswinter', 'akute', 'gegliederte', 'kurie', 'zungenblüten', 'bakterienstamm', 'unverwechselbaren', 'leck', 'wertebereich', 'sehenswerten', 'mirror', 'kologischen', 'augusto', 'knolle', 'leistungsniveau', 'coda', 'augenlider', 'ovale', 'gabriela', 'gepanzerte', 'innenpolitik', 'bronzerang', 'sarkophage', 'bestplatzierten', 'tillmann', 'baugewerbe', 'auszunutzen', 'bischofssitzes', 'verlustreichen', 'mons', 'unbefestigte', 'pritschenwagen', 'ausgewiesenen', 'erließ', 'ratifizierungsprozess', 'frust', 'pastoren', 'unternehmensgeschichte', 'gesetzmäßigkeit', 'gefäßen', 'urteils', 'christin', 'kegelförmige', 'flutwelle', 'handelsregister', 'resultierten', 'parodien', 'ladislaus', 'hewitt', 'superstar', 'strobel', 'karikatur', 'coldstream', 'guards', 'lebensaufgabe', 'bergmassiv', 'purcell', 'schwaiger', 'isles', 'satzbau', 'gerichtes', 'verblüfft', 'indirekter', 'sauna', 'timbuktu', 'vorprogrammiert', 'diät', 'degradiert', 'schur', 'prima', 'ligapokal', 'zitiere', 'sagan', 'provider', 'werther', 'vergeblichen', 'aufwärtstrend', 'fortsetzt']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words_4 = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] == 4:\n",
    "        train_rare_words_4.append(word)\n",
    "        \n",
    "print(len(train_rare_words_4))\n",
    "print(train_rare_words_4[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4604\n",
      "['besänftigen', 'erskine', 'unbeeinflusst', 'sejm', 'sprachgruppe', 'resultaten', 'geschäftspartner', 'thing', 'feindschaft', 'aromatische', 'platzgründen', 'feigen', 'erzieherin', 'statements', 'autoritäre', 'arendt', 'gunther', 'markthalle', 'kristallinen', 'infizierte', 'einzelausstellungen', 'überstimmt', 'trockenes', 'oberschenkel', 'betonten', 'jahrgangsstufe', 'mangas', 'highschoolzeit', 'lam', 'anfallen', 'brass', 'erler', 'kosaken', 'menzel', 'kriegsgräberfürsorge', 'barbaro', 'bindestrich', 'stiefel', 'wiedereintritt', 'austrieb', 'stängels', 'fidschi', 'galloway', 'eisenbahnverbindungen', 'aprikosen', 'biosynthese', 'dargebracht', 'einstweiligen', 'neugründungen', 'gräberfelder', 'volksversammlung', 'urbane', 'unverkennbar', 'brikettfabrik', 'kavallerieregiment', 'funkhaus', 'eg', 'amtlich', 'anteilen', 'flag', 'tierversuchen', 'ausschliesslich', 'mächtiger', 'unvollständigen', 'beigefügt', 'klagt', 'socken', 'stadtpfarrer', 'gemeinsamkeit', 'landesverwaltung', 'durchgangsverkehr', 'völkerrechtlichen', 'innenpolitisch', 'waldungen', 'unzutreffend', 'ernsthafter', 'pays', 'weinbauregion', 'maulwurf', 'bewässerten', 'vorgegebene', 'gesteinen', 'zusammenhängenden', 'basierende', 'heimstadion', 'getrocknete', 'meteorologische', 'frischer', 'ventures', 'bauchschmerzen', 'notausgang', 'wachtmeister', 'borsig', 'lichtschranke', 'schreibe', 'verwaltungsgerichtshof', 'doppeltitel', 'spielberechtigt', 'listenplatz', 'religionswissenschaft']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words_5 = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] == 5:\n",
    "        train_rare_words_5.append(word)\n",
    "        \n",
    "print(len(train_rare_words_5))\n",
    "print(train_rare_words_5[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1284\n",
      "['bertha', 'dänischer', 'verdankte', 'grass', 'herrschten', 'informationsgesellschaft', 'hallen', 'schleusen', 'hinterlegt', 'praktikabel', 'soliden', 'handwerks', 'bremerhaven', 'kirchenschiffs', 'verkehrswege', 'überwiegenden', 'gebäudekomplex', 'karosserieform', 'bezirksfreien', 'feststellbar', 'vertikal', 'begleiteten', 'getränk', 'terminen', 'unternehmungen', 'zusatzstoffe', 'klimatischen', 'möller', 'eisenerz', 'musiktheater', 'auktionen', 'wood', 'vitorino', 'süditalien', 'sauerland', 'griffin', 'überlieferten', 'totalen', 'mose', 'schleswigholsteins', 'bishop', 'zufahrt', 'zögerlich', 'hendrix', 'findest', 'einschränken', 'hauptinsel', 'floss', 'schulischen', 'fairness', 'dubai', 'inhalts', 'quatsch', 'größenordnung', 'industriekultur', 'ausreden', 'veronika', 'festgesetzt', 'verzweiflung', 'falsches', 'bernardo', 'therapien', 'beklagt', 'altäre', 'ostfriesland', 'überlappen', 'seitental', 'bankkaufmann', 'züchter', 'flüchteten', 'bringe', 'klausel', 'thüringischen', 'passwort', 'länglichen', 'landgraf', 'versehentlich', 'fahnen', 'grablege', 'ungefährlich', 'defekt', 'korb', 'tiefste', 'staatswissenschaften', 'nachfahre', 'sahne', 'organisatorische', 'gemieden', 'reichenau', 'totes', 'einfließen', 'befruchtung', 'armenischen', 'schiene', 'aircraft', 'wohlhabende', 'bekleidung', 'moser', 'pfeil', 'geschlechtsdimorphismus']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words_10 = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] == 10:\n",
    "        train_rare_words_10.append(word)\n",
    "        \n",
    "print(len(train_rare_words_10))\n",
    "print(train_rare_words_10[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Stem Words: 91121\n",
      "Rare stem Words freq 83273\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "train_rare_stem_words = []\n",
    "\n",
    "for word in train_rare_words:\n",
    "    train_rare_stem_words.append(stemmer.stem(word))\n",
    "\n",
    "print(\"Rare Stem Words:\", len(train_rare_stem_words))\n",
    "\n",
    "train_rare_stem_words_freq = Counter(train_rare_stem_words)\n",
    "\n",
    "print(\"Rare stem Words freq\", len(train_rare_stem_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drannehm', 'marktverstandnis', 'heilbad', 'muttersohnkonflikt', 'alfried', 'abrakadabra', 'jadebus', 'rohstoffknapp', 'privatkraftwag', 'indoar', 'schieb', 'kartenles', 'juniorengrandprixfinal', 'platzanweis', 'abkauft', 'fehlerkorrektur', 'franzoseneinfall', 'knollig', 'obi', 'maissort', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoff', 'stahlfelg', 'tankdeckel', 'gerst', 'bestreicht', 'brotchenhalft', 'ausgebeutet', 'ausbeut', 'morder', 'verhandlungssach', 'luftkraftstoffgem', 'emil', 'gom', 'nassauidenstein', 'logoireih', 'hereinhol', 'dunenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'tauernfenst', 'gerichtsakt', 'funston', 'bamberg', 'czapski', 'staatsuniversitat', 'vallast', 'gifsuryvett', 'tuning', 'knoll', 'nauroth', 'dietenhof', 'armen', 'kunstgewerbemuseum', 'militarwaff', 'lgotz', 'kinderdentist', 'zeigeg', 'korrekt', 'dervi', 'erolu', 'monogam', 'vogl', 'handelsgeschaft', 'bohlau', 'quellenfrei', 'gnesiolutheran', 'philippist', 'zugerechnet', 'herzjesuverehr', 'schutzumschlag', 'libris', 'savoiamarchetti', 'sudatlantikluftverkehr', 'saintimi', 'pquier', 'gameplays', 'anlegestell', 'doktorexam', 'verschaffelt', 'kristallographi', 'verdunkelungsgefahr', 'tonleit', 'zigeunerdurtonleit', 'kontrabassist', 'remich', 'grevenmach', 'poo', 'verbundsicherheitsglas', 'windschutzscheib', 'dnepr', 'vogon', 'einsprach', 'homilet', 'kennedyra']\n"
     ]
    }
   ],
   "source": [
    "train_rare_stem_words_2 = []\n",
    "\n",
    "train_rare_stem_words_freq_keys = train_rare_stem_words_freq.keys()\n",
    "\n",
    "for word in train_rare_stem_words_freq:\n",
    "    if train_rare_stem_words_freq[word] <= 1:\n",
    "        train_rare_stem_words_2.append(word)\n",
    "        \n",
    "print(train_rare_stem_words_2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123944\n",
      "66504\n"
     ]
    }
   ],
   "source": [
    "print(len(train_word_stem_freq))\n",
    "\n",
    "rare_stem_words = []\n",
    "\n",
    "train_word_stem_freq_keys = train_word_stem_freq.keys()\n",
    "\n",
    "for word in train_word_stem_freq:\n",
    "    if train_word_stem_freq[word] < 2:\n",
    "        rare_stem_words.append(word)\n",
    "        \n",
    "print(len(rare_stem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120003\n",
      "65216\n"
     ]
    }
   ],
   "source": [
    "train_word_stem_lemma_freq = Counter(train_words_stemmed_lemma_List)\n",
    "\n",
    "print(len(train_word_stem_lemma_freq))\n",
    "\n",
    "rare_stem_lemma_words = []\n",
    "\n",
    "train_word_stem_freq_keys = train_word_stem_lemma_freq.keys()\n",
    "\n",
    "for word in train_word_stem_lemma_freq:\n",
    "    if train_word_stem_lemma_freq[word] < 2:\n",
    "        rare_stem_lemma_words.append(word)\n",
    "        \n",
    "print(len(rare_stem_lemma_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words = [token.text\n",
    "         for token in val_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words_trf = [token.text\n",
    "         for token in val_doc_trf\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(val_words == val_words_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lemma = [token.lemma_\n",
    "         for token in val_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val lemma:  68675\n",
      "Val words dict:  26350\n"
     ]
    }
   ],
   "source": [
    "print(\"Val lemma: \", len(val_lemma))\n",
    "val_lemma_freq = Counter(val_lemma)\n",
    "print(\"Val words dict: \", len(val_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18099\n",
      "['feick', 'geflügel', 'schweinezucht', 'beschäftigungen', 'okci', 'fischingen', 'westsahara', 'kisten', 'protein', 'lebenswichtig', 'kostüm', 'hingucker', 'frühchen', 'postkarte', 'christiane', 'matheprüfung', 'mithelfen', 'maisfeldes', 'labyrinth', 'aufzubocken', 'tschüss', 'streunend', 'kater', 'massenweise', 'spatzen', 'frostschutzmittel', 'schnupfen', 'krukenbergspindel', 'saintdenis', 'kaspar', 'wolfersweiler', 'unfalls', 'schlackerst', 'merlin', 'fleisch', 'fleischwolf', 'aufgeditscht', 'haushaltsbeschränkung', 'kelsos', 'harfe', 'auffassungen', 'rauschgifthandels', 'stabhochsprung', 'enttäuschung', 'doppelkabine', 'burmanniaarten', 'daddeln', 'schwielen', 'headliner', 'saarlouis', 'abschnittweise', 'apotheker', 'geschlechtsdimorphismus', 'frühstückt', 'personalleiter', 'ausnahmetalent', 'unzüchtiges', 'krokodilklemmen', 'ankurbeln', 'ladenbesitzer', 'schiebetür', 'zahnlücke', 'ausflippen', 'erwidern', 'olga', 'ernten', 'säen', 'umsiedeln', 'fix', 'konstante', 'landbesitz', 'werkzeugen', 'ersatzteilen', 'nahrungsmangel', 'marienkäferlarven', 'wanderungsbilanz', 'köpke', 'eintracht', 'ramona', 'kali', 'liiert', 'tenplatz', 'primitiv', 'vielzeller', 'klempner', 'takoradi', 'palais', 'steinmaur', 'römern', 'fortbewegung', 'paddeln', 'bildungssprache', 'estlands', 'liturgisch', 'agrarwissenschaften', 'diplomlandwirt', 'obdachlosenasyl', 'gemeindegebiets', 'langjökull', 'wohnanschrift']\n"
     ]
    }
   ],
   "source": [
    "rare_val_lemma = []\n",
    "\n",
    "val_lemma_freq_keys = val_lemma_freq.keys()\n",
    "\n",
    "for word in val_lemma_freq_keys:\n",
    "    if val_lemma_freq[word] <= 1:\n",
    "        rare_val_lemma.append(word)\n",
    "\n",
    "print(len(rare_val_lemma))\n",
    "print(rare_val_lemma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val words:  68675\n",
      "Val words dict:  29735\n"
     ]
    }
   ],
   "source": [
    "print(\"Val words: \", len(val_words))\n",
    "val_words_freq = Counter(val_words)\n",
    "print(\"Val words dict: \", len(val_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20551\n",
      "['feick', 'geflügel', 'schweinezucht', 'beschäftigungen', 'okci', 'fischingen', 'westsahara', 'kisten', 'stopfen', 'protein', 'lebenswichtig', 'kostüm', 'hingucker', 'frühchen', 'errechneten', 'postkarte', 'christiane', 'matheprüfung', 'mitgeholfen', 'maisfeldes', 'labyrinth', 'aufzubocken', 'tschüss', 'streunender', 'kater', 'massenweise', 'spatzen', 'natürlichem', 'frostschutzmittel', 'harmloser', 'schnupfen', 'krukenbergspindel', 'saintdenis', 'kaspar', 'wolfersweiler', 'unfalls', 'abfällt', 'schlackerst', 'merlin', 'fleisch', 'fleischwolf', 'versunken', 'aufgeditscht', 'haushaltsbeschränkung', 'einzubinden', 'kelsos', 'fremde', 'harfe', 'auffassungen', 'rauschgifthandels', 'stabhochsprung', 'enttäuschung', 'doppelkabine', 'burmanniaarten', 'daddeln', 'schwielen', 'headliner', 'saarlouis', 'abschnittweise', 'apotheker', 'ausgeprägter', 'geschlechtsdimorphismus', 'frühstückt', 'wundern', 'personalleiter', 'ausnahmetalent', 'unzüchtiges', 'krokodilklemmen', 'angekurbelt', 'ladenbesitzer', 'merkte', 'freuten', 'schiebetür', 'einklemmte', 'zahnlücke', 'auszuflippen', 'erwiderte', 'olga', 'erntet', 'sät', 'umliegende', 'umgesiedelt', 'fixe', 'konstante', 'landbesitz', 'werkzeugen', 'ersatzteilen', 'nahrungsmangel', 'marienkäferlarven', 'wanderungsbilanz', 'köpke', 'eintracht', 'ramona', 'kali', 'liiert', 'tenplatz', 'primitivsten', 'vielzeller', 'klempner', 'verharmlosen']\n"
     ]
    }
   ],
   "source": [
    "rare_val_words = []\n",
    "\n",
    "val_words_freq_keys = val_words_freq.keys()\n",
    "\n",
    "for word in val_words_freq_keys:\n",
    "    if val_words_freq[word] <= 1:\n",
    "        rare_val_words.append(word)\n",
    "\n",
    "print(len(rare_val_words))\n",
    "print(rare_val_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232\n"
     ]
    }
   ],
   "source": [
    "val_train_set = []\n",
    "for word in train_rare_words:\n",
    "    if val_words_freq[word] >= 1:\n",
    "        val_train_set.append(word)\n",
    "        \n",
    "print(len(val_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [token.text\n",
    "         for token in test_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all:  140713\n",
      "Test all dict:  30042\n"
     ]
    }
   ],
   "source": [
    "test_all = [token.text\n",
    "         for token in test_doc\n",
    "         if not token.is_punct]\n",
    "\n",
    "print(\"Test all: \", len(test_all))\n",
    "test_all_freq = Counter(test_all)\n",
    "print(\"Test all dict: \", len(test_all_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words:  67545\n",
      "Test words dict:  29553\n"
     ]
    }
   ],
   "source": [
    "print(\"Test words: \", len(test_words))\n",
    "test_words_freq = Counter(test_words)\n",
    "print(\"Test words dict: \", len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3235\n",
      "3418\n"
     ]
    }
   ],
   "source": [
    "test_train_set = []\n",
    "test_train_count = 0\n",
    "for word in train_rare_words:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count = test_train_count + test_words_freq[word]\n",
    "        test_train_set.append(word)\n",
    "        \n",
    "print(len(test_train_set))\n",
    "print(test_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239\n",
      "2446\n"
     ]
    }
   ],
   "source": [
    "test_train_set_2 = []\n",
    "test_train_count_2 = 0\n",
    "for word in train_rare_words_2:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_2 = test_train_count_2 + test_words_freq[word]\n",
    "        test_train_set_2.append(word)\n",
    "        \n",
    "print(len(test_train_set_2))\n",
    "print(test_train_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n",
      "1758\n"
     ]
    }
   ],
   "source": [
    "test_train_set_3 = []\n",
    "test_train_count_3 = 0\n",
    "for word in train_rare_words_3:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_3 = test_train_count_3 + test_words_freq[word]\n",
    "        test_train_set_3.append(word)\n",
    "        \n",
    "print(len(test_train_set_3))\n",
    "print(test_train_count_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drüben\n",
      "eingezeichnet\n",
      "packung\n",
      "1280\n",
      "1486\n"
     ]
    }
   ],
   "source": [
    "test_train_set_4 = []\n",
    "test_train_count_4 = 0\n",
    "for word in train_rare_words_4:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_4 = test_train_count_4 + test_words_freq[word]\n",
    "        test_train_set_4.append(word)\n",
    "    if test_words_freq[word] >= 4:\n",
    "        print(word)\n",
    "        \n",
    "print(len(test_train_set_4))\n",
    "print(test_train_count_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "test_train_set_5 = []\n",
    "test_train_count_5 = 0\n",
    "for word in train_rare_words_5:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_5 = test_train_count_5 + test_words_freq[word]\n",
    "        test_train_set_5.append(word)\n",
    "        \n",
    "print(len(test_train_set_5))\n",
    "print(test_train_count_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bremerhaven\n",
      "sahne\n",
      "enzyme\n",
      "geste\n",
      "regierungssitz\n",
      "bewaffnet\n",
      "teppich\n",
      "erwachsen\n",
      "versteckte\n",
      "segen\n",
      "isst\n",
      "erzählte\n",
      "verbraucht\n",
      "pole\n",
      "warmen\n",
      "angetroffen\n",
      "549\n",
      "808\n"
     ]
    }
   ],
   "source": [
    "test_train_set_10 = []\n",
    "test_train_count_10 = 0\n",
    "for word in train_rare_words_10:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_10 = test_train_count_10 + test_words_freq[word]\n",
    "        test_train_set_10.append(word)\n",
    "    if test_words_freq[word] >= 4:\n",
    "        print(word)\n",
    "        \n",
    "print(len(test_train_set_10))\n",
    "print(test_train_count_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris/checkpoint-51000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    input_values = processor(\n",
    "          batch[\"speech\"], \n",
    "          sampling_rate=batch[\"sampling_rate\"], \n",
    "          return_tensors=\"pt\"\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "test_sampled = load_from_disk(\"E:/Master/data/test_sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55068759ac944cd1be7b28b519ad8ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15588 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = test_sampled.map(map_to_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.147\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.save_to_disk(\"results51_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [06:43<00:00, 38.67it/s]\n"
     ]
    }
   ],
   "source": [
    "result_complete_text = \"\"\n",
    "for i in tqdm(range(results.shape[0])):\n",
    "    result_complete_text = result_complete_text + results[i]['pred_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc = nlp(result_complete_text, disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc_trf = nlp(result_complete_text, disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(result_doc_trf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result words:  66319\n",
      "Result words dict:  40662\n"
     ]
    }
   ],
   "source": [
    "result_words = [token.text\n",
    "         for token in result_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "\n",
    "print(\"Result words: \", len(result_words))\n",
    "result_words_freq = Counter(result_words)\n",
    "print(\"Result words dict: \", len(result_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result words:  123501\n",
      "Result words dict:  41131\n"
     ]
    }
   ],
   "source": [
    "result_all = [token.text\n",
    "         for token in result_doc\n",
    "         if not token.is_punct]\n",
    "\n",
    "print(\"Result words: \", len(result_all))\n",
    "result_all_freq = Counter(result_all)\n",
    "print(\"Result words dict: \", len(result_all_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words dict:  29553\n",
      "17017\n"
     ]
    }
   ],
   "source": [
    "print(\"Test words dict: \", len(test_words_freq))\n",
    "\n",
    "test_words_freq_keys = test_words_freq.keys()\n",
    "\n",
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in test_train_set:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in test_train_set_2:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in test_train_set_3:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in test_train_set_4:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in test_train_set_5:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in test_train_set_10:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
