{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kenlm\n",
    "#import ctcdecode\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, load_metric, Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris/checkpoint-51000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1gram\n",
      "found 2gram\n"
     ]
    }
   ],
   "source": [
    "alpha = 2.5 # LM Weight\n",
    "beta = 0.0 # LM Usage Reward\n",
    "vocab = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_']\n",
    "\n",
    "word_lm_scorer = ctcdecode.WordKenLMScorer('lm_data/train.arpa', alpha, beta) # use your own kenlm model\n",
    "\n",
    "decoder = ctcdecode.BeamSearchDecoder(\n",
    "    vocab,\n",
    "    num_workers=2,\n",
    "    beam_width=128,\n",
    "    scorers=[word_lm_scorer],\n",
    "    cutoff_prob=np.log(0.000001),\n",
    "    cutoff_top_n=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = tokenizer.get_vocab()\n",
    "\n",
    "sort_vocab = sorted((value, key) for (key,value) in vocab_dict.items())\n",
    "vocab = [x[1].replace(\"|\", \" \") if x[1] not in tokenizer.all_special_tokens else \"_\" for x in sort_vocab]\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'm': 1, 'ö': 2, 'v': 3, 'p': 4, 'y': 5, 'z': 6, 'f': 7, 'd': 8, 'j': 9, 'i': 10, 't': 11, 'r': 12, 'ä': 13, 'n': 14, 'w': 15, 'h': 16, 'l': 17, 'u': 18, 'a': 19, 'x': 20, 's': 21, 'b': 22, 'c': 23, 'ß': 24, 'ü': 25, 'e': 26, 'g': 27, 'q': 28, 'k': 30, '|': 29, '[UNK]': 31, '[PAD]': 32}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dict)\n",
    "vocab_dict[' '] = vocab_dict['|']\n",
    "del vocab_dict['|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'm': 1, 'ö': 2, 'v': 3, 'p': 4, 'y': 5, 'z': 6, 'f': 7, 'd': 8, 'j': 9, 'i': 10, 't': 11, 'r': 12, 'ä': 13, 'n': 14, 'w': 15, 'h': 16, 'l': 17, 'u': 18, 'a': 19, 'x': 20, 's': 21, 'b': 22, 'c': 23, 'ß': 24, 'ü': 25, 'e': 26, 'g': 27, 'q': 28, 'k': 30, '[UNK]': 31, '[PAD]': 32, ' ': 29}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "labels = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_']\n",
    "\n",
    "decoder = CTCBeamDecoder(\n",
    "    labels,\n",
    "    model_path='lm_data/train.arpa',\n",
    "    alpha=1,\n",
    "    beta=0,\n",
    "    cutoff_top_n=100,\n",
    "    cutoff_prob=np.log(0.000001),\n",
    "    beam_width=128,\n",
    "    num_processes=16,\n",
    "    blank_id=32,\n",
    "    log_probs_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'm': 1, 'ö': 2, 'v': 3, 'p': 4, 'y': 5, 'z': 6, 'f': 7, 'd': 8, 'j': 9, 'i': 10, 't': 11, 'r': 12, 'ä': 13, 'n': 14, 'w': 15, 'h': 16, 'l': 17, 'u': 18, 'a': 19, 'x': 20, 's': 21, 'b': 22, 'c': 23, 'ß': 24, 'ü': 25, 'e': 26, 'g': 27, 'q': 28, 'k': 30, '|': 29, '[UNK]': 31, '[PAD]': 32}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ihre fotostrecken erschienen mit modemagazin wie der wolgaostseeka\n"
     ]
    }
   ],
   "source": [
    "beam_results, beam_scores, timesteps, out_len = decoder.decode(torch.tensor([logs]))\n",
    "\n",
    "inv_map = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "res = \"\"\n",
    "for n in beam_results[0][0][:out_len[0][0]]:\n",
    "    res = res + inv_map[int(n)]\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_chunk = load_from_disk(\"small_chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_chunk = Dataset.from_dict(res51_full_log[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 33)\n",
      "ihre fortestrecken erschienen mit molemagazine wie der wog at das basarwaryclar\n",
      "ihre forte strecken erschienen mit modemagazin wie der vogtes basar barrique \n",
      "ihre fotostrecken erschienen in modemagazinen wie der vogue harpers bazaar und marie claire \n"
     ]
    }
   ],
   "source": [
    "logs = res51_full_log[2]['lm_raw']\n",
    "pred_text = res51_full_log[2]['pred_str']\n",
    "lm_text = res51_full_log[2]['lm_str']\n",
    "tar_text = res51_full_log[2]['target_text']\n",
    "print(np.asarray(logs).shape)\n",
    "print(pred_text)\n",
    "print(text)\n",
    "print(tar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 33)\n",
      "ihre fortestrecken erschienen mit molemagazine wie der wog at das basarwaryclar\n",
      "ihre forte strecken erschienen mit modemagazin wie der vogtes basar barrique \n",
      "ihre fotostrecken erschienen in modemagazinen wie der vogue harpers bazaar und marie claire \n"
     ]
    }
   ],
   "source": [
    "logs = small_chunk[2]['lm_raw']\n",
    "pred_text = small_chunk[2]['pred_str']\n",
    "lm_text = small_chunk[2]['lm_str']\n",
    "tar_text = small_chunk[2]['target_text']\n",
    "print(np.asarray(logs).shape)\n",
    "print(pred_text)\n",
    "print(lm_text)\n",
    "print(tar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    input_values = processor(\n",
    "          batch[\"speech\"], \n",
    "          sampling_rate=batch[\"sampling_rate\"], \n",
    "          return_tensors=\"pt\"\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    \n",
    "    batch[\"lm_raw_2\"] = logits.cpu().numpy()\n",
    "    \n",
    "    #batch[\"lm_str\"] = decode(logits[0].cpu().numpy())\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_lm(batch):\n",
    "    \n",
    "    batch[\"lm_str\"] = decode(np.asarray(batch[\"lm_raw\"]))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix(\"\", 0.0, \"None\", 0.0, -inf)\n"
     ]
    }
   ],
   "source": [
    "from ctcdecode.prefix import State\n",
    "\n",
    "# Initialize prefixes\n",
    "prefixes = State(\n",
    "    scorers=[word_lm_scorer],\n",
    "    size=128\n",
    ")\n",
    "\n",
    "for prefix in prefixes:\n",
    "    print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "92\n",
      "ihre fotostrecken erschienen in modemagazinen wie der vogue harpers bazaar und marie claire \n"
     ]
    }
   ],
   "source": [
    "logs = res51_full_log[2]['lm_raw']\n",
    "text = res51_full_log[2]['lm_str']\n",
    "text = res51_full_log[2]['target_text']\n",
    "print(len(logs))\n",
    "print(len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "logs = np.asarray(logs)\n",
    "nT = logs.shape[0]\n",
    "print(nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruned_vocab_indices(log_probs):\n",
    "    \n",
    "    cutoff_prob=np.log(0.000001)\n",
    "    cutoff_top_n=40\n",
    "    \n",
    "    \"\"\" Return vocab indices of pruned probabilities of a time step. \"\"\"\n",
    "\n",
    "    index_to_prob = [(k, log_probs[k]) for k in range(log_probs.shape[0])]\n",
    "    index_to_prob = sorted(index_to_prob, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if cutoff_top_n < len(index_to_prob):\n",
    "        index_to_prob = index_to_prob[:cutoff_top_n]\n",
    "\n",
    "    if cutoff_prob < 1.0:\n",
    "        filtered = []\n",
    "        for x in index_to_prob:\n",
    "            if x[1] >= cutoff_prob:\n",
    "                filtered.append(x)\n",
    "        index_to_prob = filtered\n",
    "\n",
    "    return [x[0] for x in index_to_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "\n",
      "33\n",
      "[ -3.14541698  -3.76524782  -6.66179562  -5.16091967  -3.92799187\n",
      "  -4.91117573  -2.58480668  -4.42430305   0.25773394  -5.30773211\n",
      "  -1.49122858  -2.32450509  -4.87767696  -5.76897573  -2.33950615\n",
      "  -3.94859457  -2.17412162  -3.89775801  -2.74408746  -1.11488378\n",
      "  -5.61254215  -2.56076002  -2.87019825  -3.36453319  -6.88686419\n",
      "  -6.19530964  -0.64040613  -3.38129091  -6.03537321  -3.85242057\n",
      "  -3.87596941 -17.70882416  11.8444109 ]\n",
      "32\n",
      "[32, 8, 26, 19, 10, 16, 11, 14, 21, 6, 18, 22, 0, 23, 27, 1, 29, 30, 17, 4, 15, 7, 12, 5, 3, 9, 20, 13, 28, 25, 2, 24]\n",
      "\n",
      "33\n",
      "[ -3.36899519  -4.43510294  -6.61256313  -6.11390257  -4.59750032\n",
      "  -4.83982563  -3.82120395  -5.33151674  -1.2708199   -5.74136782\n",
      "  -1.47957909  -3.00136232  -4.17404079  -6.08344412  -2.56120777\n",
      "  -4.98186874  -2.53568602  -3.56372023  -3.00162482  -1.67863286\n",
      "  -6.66186666  -3.39269567  -3.80653071  -3.3572135   -7.5221324\n",
      "  -6.34197712  -1.40185916  -3.97075295  -6.73809624  -3.15032697\n",
      "  -4.36318111 -19.58986282  13.28065872]\n",
      "32\n",
      "[32, 8, 26, 10, 19, 16, 14, 11, 18, 29, 23, 0, 21, 17, 22, 6, 27, 12, 30, 1, 4, 5, 15, 7, 9, 13, 3, 25, 2, 20, 28, 24]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "for t in range(2):\n",
    "    print(\"\")\n",
    "    step_probs = logs[t]\n",
    "    print(len(step_probs))\n",
    "    print(step_probs)\n",
    "    pruned_step_probs = get_pruned_vocab_indices(step_probs)\n",
    "    print(len(pruned_step_probs))\n",
    "    print(pruned_step_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________________i_h__ree_ __f__o_r__t__e____s_tr_e_c____ke__n  __e_r_sscch_i_ee_nn_en_  mi___t_ _m_o_____le____m_a____g__a______z__i____nne_ _w_ie_ d_e_r  _w____o______g___ __________a___t_ _d_a__s__ _b__a_____s_a_____r______w__a____r__y_______c__l_a____r__________________________________________________ \n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for t in range(nT):\n",
    "    step_probs = logs[t]\n",
    "    max_index = step_probs.argmax()\n",
    "    symbol = vocab[max_index]\n",
    "    result = result + symbol\n",
    "\n",
    "print(result)\n",
    "    #pruned_step_probs = get_pruned_vocab_indices(step_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_lm(batch, word_lm_scorer, vocabulary):\n",
    "    from ctcdecode.prefix import State\n",
    "    import numpy as np\n",
    "\n",
    "    def get_pruned_vocab_indices(log_probs):\n",
    "        \"\"\" Return vocab indices of pruned probabilities of a time step. \"\"\"\n",
    "\n",
    "        index_to_prob = [(k, log_probs[k]) for k in range(log_probs.shape[0])]\n",
    "        index_to_prob = sorted(index_to_prob, key=lambda x: x[1], reverse=True)\n",
    "        if 40 < len(index_to_prob):\n",
    "            index_to_prob = index_to_prob[:40]\n",
    "\n",
    "        if np.log(0.000001) < 1.0:\n",
    "            filtered = []\n",
    "            for x in index_to_prob:\n",
    "                if x[1] >= np.log(0.000001):\n",
    "                    filtered.append(x)\n",
    "            index_to_prob = filtered\n",
    "\n",
    "        return [x[0] for x in index_to_prob]\n",
    "\n",
    "    def decode(probs):\n",
    "        # Num time steps\n",
    "        nT = probs.shape[0]\n",
    "\n",
    "        # Initialize prefixes\n",
    "        prefixes = State(\n",
    "            scorers=[word_lm_scorer],\n",
    "            size=128\n",
    "        )\n",
    "\n",
    "        # Iterate over timesteps\n",
    "        for t in range(nT):\n",
    "            step_probs = probs[t]\n",
    "            pruned_step_probs = get_pruned_vocab_indices(step_probs)\n",
    "\n",
    "            # Iterate over symbols\n",
    "            for v in pruned_step_probs:\n",
    "                symbol = vocabulary[v]\n",
    "                symbol_prob = step_probs[v]\n",
    "\n",
    "                # Iterate over prefixes\n",
    "                for prefix in prefixes:\n",
    "\n",
    "                    # If there is a blank, we extend the existing prefix\n",
    "                    if symbol == '_':\n",
    "                        prefix.add_p_blank(symbol_prob + prefix.score)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # If the last symbol is repeated\n",
    "                        # update the existing prefix\n",
    "                        if symbol == prefix.symbol:\n",
    "                            p = symbol_prob + prefix.p_non_blank_prev\n",
    "                            prefix.add_p_non_blank(p)\n",
    "\n",
    "                        new_prefix = prefixes.get_prefix(prefix, symbol)\n",
    "\n",
    "                        if new_prefix is not None:\n",
    "                            p = -np.inf\n",
    "\n",
    "                            if symbol == prefix.symbol and \\\n",
    "                                    prefix.p_blank_prev > -np.inf:\n",
    "                                p = prefix.p_blank_prev + symbol_prob\n",
    "\n",
    "                            elif prefix.symbol != symbol:\n",
    "                                p = prefix.score + symbol_prob\n",
    "\n",
    "                            new_prefix.add_p_non_blank(p)\n",
    "\n",
    "            prefixes.step()\n",
    "\n",
    "        prefixes.finalize()\n",
    "\n",
    "        return prefixes.best()\n",
    "\n",
    "    batch[\"lm_str\"] = decode(np.asarray(batch[\"lm_raw\"]))\n",
    "    return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
