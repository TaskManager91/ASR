{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from datasets import load_metric\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"de_dep_news_trf\")\n",
    "nlp.max_length = 17000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results51_full = load_from_disk(\"results51_test\")\n",
    "results51_cut = load_from_disk(\"results51_cut\")\n",
    "results51_cut_full = load_from_disk(\"lm_kenlm_pred/lm_cut_full\")\n",
    "results51_full_full = load_from_disk(\"lm_kenlm_pred/lm_full_full\")\n",
    "results51_cut_cut = load_from_disk(\"lm_kenlm_pred/lm_cut_cut\")\n",
    "\n",
    "results51_full_full_2 = load_from_disk(\"lm_kenlm_pred/lm_full_full_2\")\n",
    "results51_full_full_3 = load_from_disk(\"lm_kenlm_pred/lm_full_full_3\")\n",
    "\n",
    "results54_piece = load_from_disk(\"results54_piece\")\n",
    "results54_piece_lm = load_from_disk(\"lm_kenlm_pred/lm_piece_full_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Test WER: 0.147\n",
      "Cut Test WER: 0.148\n",
      "Cut --> Full:\n",
      "Cut full Test WER: 0.148\n",
      "Cut full LM Test WER: 0.134\n",
      "Full --> Full:\n",
      "Full full Test WER: 0.147\n",
      "Full full LM Test WER: 0.132\n",
      "Cut --> Cut:\n",
      "Cut cut Test WER: 0.148\n",
      "Cut cut LM Test WER: 0.256\n",
      "Full --> Full 2:\n",
      "Full full 2 Test WER: 0.147\n",
      "Full full 2 LM Test WER: 0.129\n",
      "Full --> Full 3:\n",
      "Full full 3 Test WER: 0.147\n",
      "Full full 3 LM Test WER: 0.126\n",
      "Sentencepiece:\n",
      "Sentencepiece Test WER: 0.142\n",
      "Sentencepiece LM Test WER: 0.107\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full[\"pred_str\"], references=results51_full[\"target_text\"])))\n",
    "print(\"Cut Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_cut[\"pred_str\"], references=results51_cut[\"target_text\"])))\n",
    "print(\"Cut --> Full:\")\n",
    "print(\"Cut full Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_cut_full[\"pred_str\"], references=results51_cut_full[\"target_text\"])))\n",
    "print(\"Cut full LM Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_cut_full[\"lm_str\"], references=results51_cut_full[\"target_text\"])))\n",
    "print(\"Full --> Full:\")\n",
    "print(\"Full full Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full_full[\"pred_str\"], references=results51_full_full[\"target_text\"])))\n",
    "print(\"Full full LM Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full_full[\"lm_str\"], references=results51_full_full[\"target_text\"])))\n",
    "print(\"Cut --> Cut:\")\n",
    "print(\"Cut cut Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_cut_cut[\"pred_str\"], references=results51_cut_cut[\"target_text\"])))\n",
    "print(\"Cut cut LM Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_cut_cut[\"lm_str\"], references=results51_cut_cut[\"target_text\"])))\n",
    "print(\"Full --> Full 2:\")\n",
    "print(\"Full full 2 Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full_full_2[\"pred_str\"], references=results51_full_full_2[\"target_text\"])))\n",
    "print(\"Full full 2 LM Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full_full_2[\"lm_2_str\"], references=results51_full_full_2[\"target_text\"])))\n",
    "print(\"Full --> Full 3:\")\n",
    "print(\"Full full 3 Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full_full_3[\"pred_str\"], references=results51_full_full_3[\"target_text\"])))\n",
    "print(\"Full full 3 LM Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_full_full_3[\"lm_2_str\"], references=results51_full_full_3[\"target_text\"])))\n",
    "print(\"Sentencepiece:\")\n",
    "print(\"Sentencepiece Test WER: {:.3f}\".format(wer_metric.compute(predictions=results54_piece[\"detokenized\"], references=results54_piece[\"target_text\"])))\n",
    "print(\"Sentencepiece LM Test WER: {:.3f}\".format(wer_metric.compute(predictions=results54_piece_lm[\"detokenized\"], references=results54_piece_lm[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [12:00<00:00, 21.63it/s]\n"
     ]
    }
   ],
   "source": [
    "results51_cut_full_trf = []\n",
    "for i in tqdm(range(len(results51_cut_full))):\n",
    "    results51_cut_full_trf.append(nlp(results51_cut_full[i]['lm_str'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results51_cut_full_trf, open(\"lm_data/results51_cut_full_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [11:53<00:00, 21.83it/s]\n"
     ]
    }
   ],
   "source": [
    "results51_full_full_trf = []\n",
    "for i in tqdm(range(len(results51_cut_full))):\n",
    "    results51_full_full_trf.append(nlp(results51_full_full[i]['lm_str'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results51_full_full_trf, open(\"lm_data/results51_full_full_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [11:54<00:00, 21.81it/s]\n"
     ]
    }
   ],
   "source": [
    "results51_cut_cut_trf = []\n",
    "for i in tqdm(range(len(results51_cut_cut))):\n",
    "    results51_cut_cut_trf.append(nlp(results51_cut_cut[i]['lm_str'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results51_cut_cut_trf, open(\"lm_data/results51_cut_cut_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15588/15588 [12:14<00:00, 21.23it/s]\n"
     ]
    }
   ],
   "source": [
    "results51_full_full_2_trf = []\n",
    "for i in tqdm(range(len(results51_cut_cut))):\n",
    "    results51_full_full_2_trf.append(nlp(results51_full_full_2[i]['lm_2_str'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results51_full_full_2_trf, open(\"lm_data/results51_full_full_2_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [11:56<00:00, 21.74it/s]\n"
     ]
    }
   ],
   "source": [
    "results51_full_full_3_trf = []\n",
    "for i in tqdm(range(len(results51_cut_cut))):\n",
    "    results51_full_full_3_trf.append(nlp(results51_full_full_3[i]['lm_2_str'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results51_full_full_3_trf, open(\"lm_data/results51_full_full_3_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [10:55<00:00, 23.78it/s]\n",
      "  0%|                                                                                        | 0/15588 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lm_2_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5861d1f136e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mresults54_piece_lm_trf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults54_piece_lm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mresults54_piece_lm_trf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults54_piece_lm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lm_2_str'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults54_piece_lm_trf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lm_data/results54_piece_lm_trf.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lm_2_str'"
     ]
    }
   ],
   "source": [
    "results54_piece_trf = []\n",
    "for i in tqdm(range(len(results54_piece))):\n",
    "    results54_piece_trf.append(nlp(results54_piece[i]['detokenized'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results54_piece_trf, open(\"lm_data/results54_piece_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [12:12<00:00, 21.28it/s]\n"
     ]
    }
   ],
   "source": [
    "results54_piece_lm_trf = []\n",
    "for i in tqdm(range(len(results54_piece_lm))):\n",
    "    results54_piece_lm_trf.append(nlp(results54_piece_lm[i]['detokenized'], disable = ['ner', 'parser']))\n",
    "\n",
    "pickle.dump(results54_piece_lm_trf, open(\"lm_data/results54_piece_lm_trf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results51_full_trf = pickle.load(open(\"lm_res51/spacy_res/results51_full_trf.p\", \"rb\"))\n",
    "results51_cut_trf = pickle.load(open(\"lm_data/results51_cut_trf.p\", \"rb\"))\n",
    "results51_cut_full_trf = pickle.load(open(\"lm_data/results51_cut_full_trf.p\", \"rb\"))\n",
    "results51_full_full_trf = pickle.load(open(\"lm_data/results51_full_full_trf.p\", \"rb\"))\n",
    "results51_cut_cut_trf = pickle.load(open(\"lm_data/results51_cut_cut_trf.p\", \"rb\"))\n",
    "results51_full_full_2_trf = pickle.load(open(\"lm_data/results51_full_full_2_trf.p\", \"rb\"))\n",
    "results51_full_full_3_trf = pickle.load(open(\"lm_data/results51_full_full_3_trf.p\", \"rb\"))\n",
    "\n",
    "results54_piece_trf = pickle.load(open(\"lm_data/results54_piece_trf.p\", \"rb\"))\n",
    "results54_piece_lm_trf = pickle.load(open(\"lm_data/results54_piece_lm_trf.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:14<00:00, 1103.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:14<00:00, 1092.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:14<00:00, 1089.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:14<00:00, 1109.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:13<00:00, 1123.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:13<00:00, 1139.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:13<00:00, 1122.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15588/15588 [00:13<00:00, 1147.02it/s]\n"
     ]
    }
   ],
   "source": [
    "#test_tok_full = []\n",
    "#for i in tqdm(range(len(results51_full_trf))):\n",
    "#    test_tok_full = test_tok_full + [token.text for token in results51_full_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "test_tok_cut = []\n",
    "for i in tqdm(range(len(results51_cut_trf))):\n",
    "    test_tok_cut = test_tok_cut + [token.text for token in results51_cut_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "test_tok_cut_full = []\n",
    "for i in tqdm(range(len(results51_cut_full_trf))):\n",
    "    test_tok_cut_full = test_tok_cut_full + [token.text for token in results51_cut_full_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "test_tok_full_full = []\n",
    "for i in tqdm(range(len(results51_cut_trf))):\n",
    "    test_tok_full_full = test_tok_full_full + [token.text for token in results51_full_full_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "    \n",
    "test_tok_cut_cut = []\n",
    "for i in tqdm(range(len(results51_cut_cut_trf))):\n",
    "    test_tok_cut_cut = test_tok_cut_cut + [token.text for token in results51_cut_cut_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "test_tok_full_full_2 = []\n",
    "for i in tqdm(range(len(results51_cut_trf))):\n",
    "    test_tok_full_full_2 = test_tok_full_full_2 + [token.text for token in results51_full_full_2_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "test_tok_full_full_3 = []\n",
    "for i in tqdm(range(len(results51_cut_trf))):\n",
    "    test_tok_full_full_3 = test_tok_full_full_3 + [token.text for token in results51_full_full_3_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "tok_piece = []\n",
    "for i in tqdm(range(len(results51_cut_trf))):\n",
    "    tok_piece = tok_piece + [token.text for token in results54_piece_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "\n",
    "tok_piece_lm = []\n",
    "for i in tqdm(range(len(results51_cut_trf))):\n",
    "    tok_piece_lm = tok_piece_lm + [token.text for token in results54_piece_lm_trf[i] if(not token.is_stop and not token.is_punct)]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result full words:  68467\n",
      "Result full words dict:  33810\n",
      "Result cut words:  69977\n",
      "Result cut words dict:  27273\n",
      "Result full full words:  69913\n",
      "Result full full words dict:  27363\n",
      "Result cut cut words:  74653\n",
      "Result cut cut words dict:  24362\n",
      "Result full full 2 words:  69959\n",
      "Result full full 2 words dict:  27128\n",
      "Result full full 3 words:  68898\n",
      "Result full full 3 words dict:  26992\n",
      "Result piece words:  68996\n",
      "Result piece words dict:  33369\n",
      "Result piece lm words:  67065\n",
      "Result piece lm words dict:  28169\n"
     ]
    }
   ],
   "source": [
    "print(\"Result full words: \", len(test_tok_cut))\n",
    "test_tok_cut_freq = Counter(test_tok_cut)\n",
    "print(\"Result full words dict: \", len(test_tok_cut_freq))\n",
    "\n",
    "print(\"Result cut words: \", len(test_tok_cut_full))\n",
    "test_tok_cut_full_freq = Counter(test_tok_cut_full)\n",
    "print(\"Result cut words dict: \", len(test_tok_cut_full_freq))\n",
    "\n",
    "print(\"Result full full words: \", len(test_tok_full_full))\n",
    "test_tok_full_full_freq = Counter(test_tok_full_full)\n",
    "print(\"Result full full words dict: \", len(test_tok_full_full_freq))\n",
    "\n",
    "print(\"Result cut cut words: \", len(test_tok_cut_cut))\n",
    "test_tok_cut_cut_freq = Counter(test_tok_cut_cut)\n",
    "print(\"Result cut cut words dict: \", len(test_tok_cut_cut_freq))\n",
    "\n",
    "print(\"Result full full 2 words: \", len(test_tok_full_full_2))\n",
    "test_tok_full_full_2_freq = Counter(test_tok_full_full_2)\n",
    "print(\"Result full full 2 words dict: \", len(test_tok_full_full_2_freq))\n",
    "\n",
    "print(\"Result full full 3 words: \", len(test_tok_full_full_3))\n",
    "test_tok_full_full_3_freq = Counter(test_tok_full_full_3)\n",
    "print(\"Result full full 3 words dict: \", len(test_tok_full_full_3_freq))\n",
    "\n",
    "print(\"Result piece words: \", len(tok_piece))\n",
    "tok_piece_freq = Counter(tok_piece)\n",
    "print(\"Result piece words dict: \", len(tok_piece_freq))\n",
    "\n",
    "print(\"Result piece lm words: \", len(tok_piece_lm))\n",
    "tok_piece_lm_freq = Counter(tok_piece_lm)\n",
    "print(\"Result piece lm words dict: \", len(tok_piece_lm_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_trf_loaded = pickle.load(open(\"data_w/test_doc_trf.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15588/15588 [00:16<00:00, 961.69it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tok_text = []\n",
    "for i in tqdm(range(len(test_doc_trf_loaded))):\n",
    "    test_tok_text = test_tok_text + [token.text for token in test_doc_trf_loaded[i] if(not token.is_stop and not token.is_punct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words:  67545\n",
      "Test words dict:  29553\n"
     ]
    }
   ],
   "source": [
    "print(\"Test words: \", len(test_tok_text))\n",
    "test_words_freq = Counter(test_tok_text)\n",
    "print(\"Test words dict: \", len(test_words_freq))\n",
    "\n",
    "test_words_freq_keys = test_words_freq.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> cut Pred:  0.7110614827597875\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if test_tok_cut_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> cut Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> cut full Pred:  0.731905390315704\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if test_tok_cut_full_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> cut full Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> full full Pred:  0.734680066321524\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if test_tok_full_full_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> full full Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> cut cut Pred:  0.5406219334754508\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if test_tok_cut_cut_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> cut cut Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> full full 2 Pred:  0.7393834805265117\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if test_tok_full_full_2_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> full full 2 Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> full full 3 Pred:  0.7347477413460562\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if test_tok_full_full_3_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> full full 3 Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> Piece Pred:  0.7180658477988698\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if tok_piece_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> Piece Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict: Test -> Piece LM Pred:  0.7702094542009271\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if tok_piece_lm_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(\"Dict: Test -> Piece LM Pred: \", i /len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_set = pickle.load(open(\"data/test_train_set.p\", \"rb\"))\n",
    "importantLists = [1,2,3,4,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 1 :  3235\n",
      "Number of words 2 :  2239\n",
      "Number of words 3 :  1538\n",
      "Number of words 4 :  1280\n",
      "Number of words 5 :  1063\n",
      "Number of words 6 :  912\n",
      "Number of words 7 :  766\n",
      "Number of words 8 :  647\n",
      "Number of words 9 :  638\n",
      "Number of words 10 :  549\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):      \n",
    "    print(\"Number of words \" + str(i) + \" : \", len(test_train_set[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "1819\n",
      "0.5622874806800618\n",
      "List:  2\n",
      "2239\n",
      "1357\n",
      "0.6060741402411791\n",
      "List:  3\n",
      "1538\n",
      "987\n",
      "0.6417425227568271\n",
      "List:  4\n",
      "1280\n",
      "818\n",
      "0.6390625\n",
      "List:  5\n",
      "1063\n",
      "711\n",
      "0.6688617121354656\n",
      "List:  10\n",
      "549\n",
      "424\n",
      "0.7723132969034608\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if test_tok_cut_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "2697\n",
      "0.833693972179289\n",
      "List:  2\n",
      "2239\n",
      "1872\n",
      "0.8360875390799464\n",
      "List:  3\n",
      "1538\n",
      "1309\n",
      "0.8511053315994799\n",
      "List:  4\n",
      "1280\n",
      "1101\n",
      "0.86015625\n",
      "List:  5\n",
      "1063\n",
      "925\n",
      "0.8701787394167451\n",
      "List:  10\n",
      "549\n",
      "507\n",
      "0.9234972677595629\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if test_tok_cut_full_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "2709\n",
      "0.837403400309119\n",
      "List:  2\n",
      "2239\n",
      "1917\n",
      "0.8561857972309067\n",
      "List:  3\n",
      "1538\n",
      "1333\n",
      "0.8667100130039012\n",
      "List:  4\n",
      "1280\n",
      "1095\n",
      "0.85546875\n",
      "List:  5\n",
      "1063\n",
      "934\n",
      "0.878645343367827\n",
      "List:  10\n",
      "549\n",
      "504\n",
      "0.9180327868852459\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if test_tok_full_full_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "1\n",
      "0.0003091190108191654\n",
      "List:  2\n",
      "2239\n",
      "1533\n",
      "0.6846806610093792\n",
      "List:  3\n",
      "1538\n",
      "1101\n",
      "0.7158647594278283\n",
      "List:  4\n",
      "1280\n",
      "906\n",
      "0.7078125\n",
      "List:  5\n",
      "1063\n",
      "786\n",
      "0.7394167450611477\n",
      "List:  10\n",
      "549\n",
      "430\n",
      "0.7832422586520947\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if test_tok_cut_cut_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "2719\n",
      "0.8404945904173107\n",
      "List:  2\n",
      "2239\n",
      "1911\n",
      "0.8535060294774452\n",
      "List:  3\n",
      "1538\n",
      "1345\n",
      "0.8745123537061118\n",
      "List:  4\n",
      "1280\n",
      "1101\n",
      "0.86015625\n",
      "List:  5\n",
      "1063\n",
      "939\n",
      "0.883349012229539\n",
      "List:  10\n",
      "549\n",
      "504\n",
      "0.9180327868852459\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if test_tok_full_full_2_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "2671\n",
      "0.8256568778979907\n",
      "List:  2\n",
      "2239\n",
      "1880\n",
      "0.8396605627512282\n",
      "List:  3\n",
      "1538\n",
      "1319\n",
      "0.8576072821846554\n",
      "List:  4\n",
      "1280\n",
      "1098\n",
      "0.8578125\n",
      "List:  5\n",
      "1063\n",
      "931\n",
      "0.8758231420507996\n",
      "List:  10\n",
      "549\n",
      "503\n",
      "0.9162112932604736\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if test_tok_full_full_3_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "1859\n",
      "0.5746522411128284\n",
      "List:  2\n",
      "2239\n",
      "1408\n",
      "0.6288521661456007\n",
      "List:  3\n",
      "1538\n",
      "988\n",
      "0.6423927178153446\n",
      "List:  4\n",
      "1280\n",
      "847\n",
      "0.66171875\n",
      "List:  5\n",
      "1063\n",
      "738\n",
      "0.6942615239887112\n",
      "List:  10\n",
      "549\n",
      "440\n",
      "0.8014571948998178\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if tok_piece_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:  1\n",
      "3235\n",
      "2151\n",
      "0.6649149922720248\n",
      "List:  2\n",
      "2239\n",
      "1624\n",
      "0.7253238052702099\n",
      "List:  3\n",
      "1538\n",
      "1155\n",
      "0.7509752925877763\n",
      "List:  4\n",
      "1280\n",
      "1006\n",
      "0.7859375\n",
      "List:  5\n",
      "1063\n",
      "870\n",
      "0.8184383819379115\n",
      "List:  10\n",
      "549\n",
      "490\n",
      "0.8925318761384335\n"
     ]
    }
   ],
   "source": [
    "for x in importantLists:\n",
    "    print(\"List: \", x)\n",
    "    i=0\n",
    "    print(len(test_train_set[x]))\n",
    "    for word in test_train_set[x]:\n",
    "        if tok_piece_lm_freq[word] >= 1:\n",
    "            i = i+1\n",
    "    print(i)\n",
    "    print(i/len(test_train_set[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only_words = pickle.load(open(\"data/test_only_words.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "2\n",
      "0.00034423407917383823\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if test_tok_cut_full_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "1\n",
      "0.00017211703958691912\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if test_tok_full_full_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "2802\n",
      "0.4822719449225473\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if test_tok_cut_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "2\n",
      "0.00034423407917383823\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if test_tok_cut_cut_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "86\n",
      "0.014802065404475043\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if test_tok_full_full_2_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "117\n",
      "0.020137693631669534\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if test_tok_full_full_3_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "2811\n",
      "0.48382099827882963\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if tok_piece_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "2431\n",
      "0.41841652323580036\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if tok_piece_lm_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
