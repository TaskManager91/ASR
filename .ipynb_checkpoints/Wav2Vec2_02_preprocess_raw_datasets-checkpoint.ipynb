{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pydub\n",
    "!pip install soundfile\n",
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1721d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from pathlib import Path\n",
    "\n",
    "common_voice_train = load_dataset(\"common_voice\", \"de\", split=\"train\", cache_dir=Path(\"./data\"))\n",
    "common_voice_validation = load_dataset(\"common_voice\", \"de\", split=\"validation\", cache_dir=Path(\"./data\"))\n",
    "common_voice_test = load_dataset(\"common_voice\", \"de\", split=\"test\", cache_dir=Path(\"./data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_voice_train.shape)\n",
    "print(common_voice_validation.shape)\n",
    "print(common_voice_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_validation = common_voice_validation.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_keep = '[^A-Za-zäüöß ]+'\n",
    "\n",
    "def remove_special_characters_chris(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_keep, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters_chris)\n",
    "#common_voice_validation = common_voice_validation.map(remove_special_characters_chris)\n",
    "#common_voice_test = common_voice_test.map(remove_special_characters_chris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def speech_file_to_array_subsample_fn(batch):\n",
    "    source_path = batch[\"path\"]\n",
    "    #target_path = source_path.replace(\"/fb6c78aa77894f852352462c1b7d4734e3a9a4357d8b5cf479f1cff7c62027b8/\", \"/0471615e22737a20fe3645109147bd3f20b421f6242d7f9f7c6db670ada35454/\", 1)\n",
    "    speech_array, sampling_rate = torchaudio.load(source_path)\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"speech\"] = librosa.resample(np.asarray(batch[\"speech\"]), sampling_rate, 16_000)\n",
    "    batch[\"sampling_rate\"] = 16_000\n",
    "    batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import time\n",
    "\n",
    "def preprocess(batch):\n",
    "    \n",
    "    print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "    batch_size = 2500\n",
    "    batch_length = len(batch[\"path\"])\n",
    "    print(\"total length: \" + str(batch_length))\n",
    "    \n",
    "    result_batch = Dataset.from_dict(batch[0:batch_size])\n",
    "    result_batch = result_batch.map(speech_file_to_array_subsample_fn, remove_columns=result_batch.column_names, num_proc=8) \n",
    "    \n",
    "    for i in range(batch_size, batch_length, batch_size):\n",
    "        \n",
    "        print(\"From: \" + str(i) + \" to: \" + str(i+batch_size))\n",
    "        print(\"Time: \" + str(time.asctime()))\n",
    "        \n",
    "        small_batch = Dataset.from_dict(batch[i:i+batch_size])\n",
    "        small_batch = small_batch.map(speech_file_to_array_subsample_fn, remove_columns=small_batch.column_names, num_proc=8) \n",
    "        result_batch = concatenate_datasets([result_batch, small_batch]) \n",
    "    \n",
    "    return result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c97490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def preprocess_chunks(dataset):\n",
    "    batch_size = 10000\n",
    "    dataset_length = len(dataset[\"path\"])\n",
    "    \n",
    "    for i in range(0, dataset_length, batch_size):\n",
    "        chunk = Dataset.from_dict(dataset[i:i+batch_size])\n",
    "        chunk = preprocess(chunk)\n",
    "        chunk.save_to_disk(\"cv_sampled/data_\" + str(i) + \"_\" + str(i+batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73768cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_chunks(common_voice_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68094402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "cv_sampled_test = load_from_disk(\"cv_sampled/data_0_5000\")\n",
    "\n",
    "for file_name in glob.iglob(\"cv_sampled/*\"):\n",
    "    if(file_name ==\"cv_sampled/data_0_5000\"):\n",
    "        i= 0\n",
    "        # do nothing\n",
    "    else:\n",
    "        print(file_name)\n",
    "        cv_batch = load_from_disk(file_name)\n",
    "        cv_sampled_test = concatenate_datasets([cv_sampled_test, cv_batch])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for file_name in glob.iglob(\"G:/01-DATA/train_batch/*\"):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import glob\n",
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train = load_from_disk(\"G:/01-DATA/train_batch/train_sampled_pro_1\")\n",
    "\n",
    "first = True\n",
    "\n",
    "for file_name in glob.iglob(\"G:/01-DATA/train_batch/*\"):\n",
    "    \n",
    "    print(\"Time: \" + str(time.asctime()))\n",
    "    \n",
    "    if(first):\n",
    "        first = False\n",
    "        print(\"first one skipped\")\n",
    "    else:\n",
    "        print(file_name)\n",
    "        cv_batch = load_from_disk(file_name)\n",
    "        cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"Saving to disk!\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "cv_sampled_train.save_to_disk(\"G:\\01-DATA\\train_sampled_test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_sampled_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89895a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sampled_test.save_to_disk(\"cv_sampled_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rand_int = random.randint(0, len(cv_sampled_test)-1)\n",
    "\n",
    "print(\"Target text:\", cv_sampled_test[rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(cv_sampled_test[rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", cv_sampled_test[rand_int][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "ipd.Audio(data=np.asarray(cv_sampled_test[rand_int][\"speech\"]), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04758e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b69823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "\n",
    "train_sa_pro_2 = load_from_disk(\"D:\\Master\\wsl/5_train_sa\")\n",
    "\n",
    "print(train_sa_pro_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12049794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "train_sa_pro_2 = train_sa_pro_2.map(prepare_dataset, remove_columns=train_sa_pro_2.column_names, batch_size=64, batched=True)\n",
    "\n",
    "print(\"Save: \" + str(time.asctime()))\n",
    "\n",
    "train_sa_pro_2.save_to_disk(\"D:/Master/Data/train_sampled_pro_5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sa_pro_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled_pro = cv_sampled_test.map(prepare_dataset, remove_columns=cv_sampled_test.column_names, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa00260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sampled_pro.save_to_disk(\"test_sampled_pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "test_sampled_pro = load_from_disk(\"test_sampled_pro\")\n",
    "val_sampled_pro = load_from_disk(\"val_sampled_pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238375d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_set = concatenate_datasets([test_sampled_pro, val_sampled_pro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1945674",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_set = concatenate_datasets([big_set, val_sampled_pro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf81787",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_set.save_to_disk(\"big_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df500474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(f\"\"\"\n",
    "- Datasets: {datasets.__version__}\n",
    "- Python: {sys.version}\n",
    "- Platform: {platform.platform()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd359e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "for file_name in glob.iglob(\"D:/Master/Data/train_batch/*\"):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import glob\n",
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train = load_from_disk(\"D:/Master/Data/train_batch/train_sampled_pro_1\")\n",
    "\n",
    "first = True\n",
    "\n",
    "for file_name in glob.iglob(\"D:/Master/Data/train_batch/*\"):\n",
    "    \n",
    "    print(\"Time: \" + str(time.asctime()))\n",
    "    \n",
    "    if(first):\n",
    "        first = False\n",
    "        print(\"first one skipped\")\n",
    "    else:\n",
    "        print(\"concatenate next file\")\n",
    "        print(file_name)\n",
    "        cv_batch = load_from_disk(file_name)\n",
    "        cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"Saving to disk!\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "cv_sampled_train.save_to_disk(\"D:/Master/Data/train_sampled_pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import glob\n",
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train = load_from_disk(\"D:/Master/Data/train_batch/train_sampled_pro_1\")\n",
    "\n",
    "cv_batch = load_from_disk(\"D:/Master/Data/train_batch/train_sampled_pro_2\")\n",
    "cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"concatenate next file\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "\n",
    "cv_batch = load_from_disk(\"D:/Master/Data/train_batch/train_sampled_pro_3\")\n",
    "cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"Saving to disk!\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train.save_to_disk(\"D:/Master/Data/train_sampled_pro_big_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41829903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import glob\n",
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train = load_from_disk(\"D:/Master/Data/train_batch/train_sampled_pro_4\")\n",
    "\n",
    "cv_batch = load_from_disk(\"D:/Master/Data/train_batch/train_sampled_pro_5\")\n",
    "cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"Saving to disk!\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train.save_to_disk(\"D:/Master/Data/train_sampled_pro_big_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb63685",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sampled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8933f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import glob\n",
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "train_sampled_pro = load_from_disk(\"D:/Master/Data/train_sampled_pro_big_1\")\n",
    "\n",
    "cv_batch = load_from_disk(\"D:/Master/Data/train_sampled_pro_big_2\")\n",
    "train_sampled_pro = concatenate_datasets([train_sampled_pro, cv_batch])\n",
    "\n",
    "print(\"Saving to disk!\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "\n",
    "train_sampled_pro.save_to_disk(\"D:/Master/Data/train_sampled_pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampled_pro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ce020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import glob\n",
    "import time\n",
    "\n",
    "print(\"Start: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train = load_from_disk(\"D:\\Master\\Data/test_sampled_pro\")\n",
    "\n",
    "cv_batch = load_from_disk(\"D:\\Master\\Data/val_sampled_pro\")\n",
    "cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"concatenate next file\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "\n",
    "cv_batch = load_from_disk(\"D:\\Master\\Data/test_sampled_pro\")\n",
    "cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\n",
    "\n",
    "print(\"Saving to disk!\")\n",
    "print(\"Time: \" + str(time.asctime()))\n",
    "\n",
    "cv_sampled_train.save_to_disk(\"D:/Master/Data/test_save_BIG_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67248f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36ad41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
