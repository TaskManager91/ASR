{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "common_voice_train = load_dataset(\"common_voice\", \"de\", split=\"train\", cache_dir=\"D:\\Master\\wsl\\data\")\n",
    "common_voice_validation = load_dataset(\"common_voice\", \"de\", split=\"validation\", cache_dir=\"D:\\Master\\wsl\\data\")\n",
    "common_voice_test = load_dataset(\"common_voice\", \"de\", split=\"test\", cache_dir=\"D:\\Master\\wsl\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_validation = common_voice_validation.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_keep = '[^A-Za-zäüöß ]+'\n",
    "\n",
    "def remove_special_characters_chris(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_keep, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters_chris)\n",
    "common_voice_validation = common_voice_validation.map(remove_special_characters_chris)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters_chris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_validation.save_to_disk(\"E:/Master/data/val_text\")\n",
    "common_voice_train.save_to_disk(\"E:/Master/data/train_text\")\n",
    "common_voice_test.save_to_disk(\"E:/Master/data/test_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "train_text_dataset = load_from_disk(\"E:/Master/data/0_text/train_text\")\n",
    "val_text_dataset = load_from_disk(\"E:/Master/data/0_text/val_text\")\n",
    "test_text_dataset = load_from_disk(\"E:/Master/data/0_text/test_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy[cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf\n",
    "!python -m spacy download de_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy[transformers,cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "nlp.max_length = 17000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_text = \"\"\n",
    "val_complete_text = \"\"\n",
    "test_complete_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246525\n",
      "15588\n",
      "15588\n"
     ]
    }
   ],
   "source": [
    "print(train_text_dataset.shape[0])\n",
    "print(val_text_dataset.shape[0])\n",
    "print(test_text_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Datasets to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 246525/246525 [00:13<00:00, 18065.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 15588/15588 [00:00<00:00, 34183.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 15588/15588 [00:00<00:00, 34259.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(train_text_dataset.shape[0])):\n",
    "    train_complete_text = train_complete_text + train_text_dataset[i]['sentence']\n",
    "\n",
    "for i in tqdm(range(val_text_dataset.shape[0])):\n",
    "    val_complete_text = val_complete_text + val_text_dataset[i]['sentence']\n",
    "\n",
    "for i in tqdm(range(test_text_dataset.shape[0])):\n",
    "    test_complete_text = test_complete_text + test_text_dataset[i]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16656802\n",
      "997429\n",
      "976676\n"
     ]
    }
   ],
   "source": [
    "print(len(train_complete_text))\n",
    "print(len(val_complete_text))\n",
    "print(len(test_complete_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Datasets with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process train @  2021-05-18 14:39:00.132384\n",
      "End process train @  2021-05-18 14:40:54.267330\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process train @ \", now)\n",
    "\n",
    "train_doc = nlp(train_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process train @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process val @  2021-05-18 14:41:52.312361\n",
      "End process val @  2021-05-18 14:41:58.565515\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process val @ \", now)\n",
    "\n",
    "val_doc = nlp(val_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process val @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process test @  2021-05-18 14:42:02.360915\n",
      "End process test @  2021-05-18 14:42:08.361938\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"start process test @ \", now)\n",
    "\n",
    "test_doc = nlp(test_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process test @ \", now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tokens that arent stop words or punctuations\n",
    "train_all = [token.text\n",
    "         for token in train_doc\n",
    "         if not token.is_punct]\n",
    "\n",
    "# all tokens that arent stop words or punctuations\n",
    "train_words = [token.text\n",
    "         for token in train_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "train_nouns = [token.text\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n"
     ]
    }
   ],
   "source": [
    "for word in train_words:\n",
    "    if(word == \"ausgebeutet\"):\n",
    "        print(\"found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train all:  2350690\n",
      "train words dict:  168237\n",
      "train words:  1138469\n",
      "train words dict:  167707\n",
      "train nouns:  143335\n",
      "train nouns dict:  26890\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"train all: \", len(train_all))\n",
    "train_all_freq = Counter(train_all)\n",
    "print(\"train words dict: \", len(train_all_freq))\n",
    "\n",
    "print(\"train words: \", len(train_words))\n",
    "train_words_freq = Counter(train_words)\n",
    "print(\"train words dict: \", len(train_words_freq))\n",
    "\n",
    "print(\"train nouns: \", len(train_nouns))\n",
    "train_nouns_freq = Counter(train_nouns)\n",
    "print(\"train nouns dict: \", len(train_nouns_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma nouns test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun tokens that arent stop words or punctuations\n",
    "train_nouns_lemma = [token.lemma_\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma Words: 143335\n",
      "Lemma freq Words 26758\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemma Noun Words:\", len(train_nouns_lemma))\n",
    "\n",
    "train_nouns_lemma_freq = Counter(train_nouns_lemma)\n",
    "\n",
    "print(\"Lemma Noun freq Words\", len(train_nouns_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare Lemma words 1 :  17082\n",
      "Number of rare Lemma words 2 :  20678\n",
      "Number of rare Lemma words 3 :  22212\n",
      "Number of rare Lemma words 4 :  23109\n",
      "Number of rare Lemma words 5 :  23670\n",
      "Number of rare Lemma words 6 :  24042\n",
      "Number of rare Lemma words 7 :  24333\n",
      "Number of rare Lemma words 8 :  24578\n",
      "Number of rare Lemma words 9 :  24782\n",
      "Number of rare Lemma words 10 :  24939\n"
     ]
    }
   ],
   "source": [
    "train_nouns_lemma_freq_keys = train_nouns_lemma_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(1,n):\n",
    "    \n",
    "    rare_lemma_words = []\n",
    "    for word in train_nouns_lemma_freq_keys:\n",
    "        \n",
    "        if train_nouns_lemma_freq[word] <= i:\n",
    "            rare_lemma_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare Lemma words \" + str(i) + \" : \", len(rare_lemma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['schiebers', 'sturmes', 'piscina', 'tankdeckels', 'fertigungsanlage', 'kopie', 'tauernfensters', 'staatsuniversität', 'modellautos', 'hufen', 'rennleitung', 'schutzumschlag', 'highschoolzeit', 'kontrabassisten', 'homiletik', 'heiliggeistordens', 'beifall', 'geschichtspolitik', 'instandsetzungstruppe', 'merowingern', 'austreiben', 'sesklokultur', 'ovale', 'trajektorie', 'umsatzbeteiligung', 'gesellschafterversammlung', 'wood', 'trialog', 'granada', 'bildpunkt', 'einsatzfall', 'zarismus', 'feudalismus', 'realschulempfehlung', 'boxsack', 'versuchsanordnung', 'christin', 'xte', 'matheprüfung', 'weihnachtsbaumdekoration', 'modelltheorie', 'böhme', 'unterkieferknochen', 'ormandy', 'cholerakonferenz', 'arbeitsbewertung', 'handymusik', 'komponist', 'audiodaten', 'barchfeld', 'antonio', 'adour', 'humorvoll', 'master', 'freileitung', 'gesundheitsschutzniveau', 'ausgabenprogramm', 'schiffsunglück', 'friedenszeiten', 'mennonitengemeinde', 'leitungsfunktion', 'tritt', 'sauna', 'stiftungsprofessur', 'fernheizwerk', 'mikrofossilien', 'altenhof', 'adsorption', 'karmels', 'lebensgewohnheiten', 'passunion', 'sitzheizung', 'lombardei', 'debüts', 'südsüdhandels', 'preistheorie', 'gebäuderückseite', 'beobachter', 'lagersystem', 'babyklappe', 'felsenlandjugendherberge', 'jugendherbergswerks', 'klopapier', 'zerrung', 'fünfinfünfauspuffanlage', 'geräuschentwicklung', 'hammerschlag', 'beschleunigen', 'landtagsausschuss', 'taiwaner', 'bruchsteinsockel', 'gehhilfe', 'kreisversammlung', 'städtchens', 'lehrbuch']\n"
     ]
    }
   ],
   "source": [
    "train_rare_lemma_words = []\n",
    "for word in train_nouns_lemma_freq_keys:\n",
    "    if train_nouns_lemma_freq[word] <= 1:\n",
    "        train_rare_lemma_words.append(word)\n",
    "\n",
    "print(train_rare_lemma_words[5:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemma words test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma Words: 1138469\n",
      "Lemma freq Words 151687\n"
     ]
    }
   ],
   "source": [
    "train_words_lemma = [token.lemma_\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct)]\n",
    "\n",
    "print(\"Lemma Words:\", len(train_words_lemma))\n",
    "train_words_lemma_freq = Counter(train_words_lemma)\n",
    "print(\"Lemma freq Words\", len(train_words_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "found!\n",
      "found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in train_words:\n",
    "    if word == \"rutschte\":\n",
    "        print(\"found!\")\n",
    "\n",
    "train_words_freq['rutschte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare Lemma words 1 :  84290\n",
      "Number of rare Lemma words 2 :  108559\n",
      "Number of rare Lemma words 3 :  119610\n",
      "Number of rare Lemma words 4 :  125815\n",
      "Number of rare Lemma words 5 :  129823\n",
      "Number of rare Lemma words 6 :  132562\n",
      "Number of rare Lemma words 7 :  134655\n",
      "Number of rare Lemma words 8 :  136284\n",
      "Number of rare Lemma words 9 :  137614\n",
      "Number of rare Lemma words 10 :  138702\n"
     ]
    }
   ],
   "source": [
    "train_words_lemma_freq_keys = train_words_lemma_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(1,n):\n",
    "    \n",
    "    train_rare_words = []\n",
    "    for word in train_words_lemma_freq_keys:\n",
    "        \n",
    "        if train_words_lemma_freq[word] <= i:\n",
    "            train_rare_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare Lemma words \" + str(i) + \" : \", len(train_rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alfried', 'abrakadabra', 'abdrücken', 'jadebusen', 'rohstoffknappheit', 'privatkraftwagen', 'indoarischen', 'schiebers', 'kartenleser', 'juniorengrandprixfinale', 'platzanweiser', 'sturmes', 'abkauft', 'fehlerkorrekturen', 'franzoseneinfall', 'knollig', 'obie', 'maissorten', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoffer', 'stahlfelgen', 'rutschen', 'tankdeckels', 'gerst', 'bestreicht', 'brötchenhälfte', 'ausgebeuteten', 'ausbeuter', 'mörderisch', 'verhandlungssache', 'luftkraftstoffgemisch', 'fertigungsanlage', 'emile', 'gomer', 'nassauidenstein', 'logoireihe', 'hereinholen', 'dünenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'schiefers', 'tauernfensters', 'gerichtsakten', 'ostwestfalens', 'funston', 'bambergs', 'czapski', 'staatsuniversität', 'vallaster', 'gifsuryvette', 'tuning', 'knolls', 'nauroth', 'zeitungswissenschaft', 'dietenhofen', 'armenisch', 'kunstgewerbemuseums', 'militärwaffen', 'lgötzen', 'kinderdentist', 'hilfst', 'zeigegeste', 'korrektes', 'dervi', 'erolu', 'monogame', 'voglers', 'handelsgeschäfte', 'gefäßformen', 'böhlau', 'quellenfrei', 'ipadressen', 'gnesiolutheraner', 'philippisten', 'zugerechnete', 'lakonischen', 'herzjesuverehrung', 'rennleitung', 'schutzumschlag', 'libris', 'savoiamarchetti', 'südatlantikluftverkehr', 'saintimier', 'pquier', 'gameplays', 'alkoholikers', 'anlegestellen', 'doktorexamen', 'verschaffelt', 'kristallographie']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] <= 1:\n",
    "        train_rare_words.append(word)\n",
    "\n",
    "print(train_rare_words[5:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare words 0 :  0\n",
      "Number of rare words 1 :  91121\n",
      "Number of rare words 2 :  118265\n",
      "Number of rare words 3 :  130694\n",
      "Number of rare words 4 :  137805\n",
      "Number of rare words 5 :  142409\n",
      "Number of rare words 6 :  145643\n",
      "Number of rare words 7 :  148076\n",
      "Number of rare words 8 :  149990\n",
      "Number of rare words 9 :  151534\n",
      "Number of rare words 10 :  152818\n"
     ]
    }
   ],
   "source": [
    "train_words_freq_keys = train_words_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(0,n):\n",
    "    \n",
    "    rare_words = []\n",
    "    for word in train_words_freq_keys:\n",
    "        \n",
    "        if train_words_freq[word] <= i:\n",
    "            rare_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare words \" + str(i) + \" : \", len(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91121\n",
      "['drannehmen', 'marktverständnis', 'heilbäder', 'bildbearbeitungsprogramme', 'muttersohnkonflikt', 'alfried', 'abrakadabra', 'abdrücken', 'jadebusen', 'rohstoffknappheit', 'privatkraftwagen', 'indoarischen', 'schiebers', 'kartenleser', 'juniorengrandprixfinale', 'platzanweiser', 'sturmes', 'abkauft', 'fehlerkorrekturen', 'franzoseneinfall', 'knollig', 'obie', 'maissorten', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoffer', 'stahlfelgen', 'rutschen', 'tankdeckels', 'gerst', 'bestreicht', 'brötchenhälfte', 'ausgebeuteten', 'ausbeuter', 'mörderisch', 'verhandlungssache', 'luftkraftstoffgemisch', 'fertigungsanlage', 'emile', 'gomer', 'nassauidenstein', 'logoireihe', 'hereinholen', 'dünenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'schiefers', 'tauernfensters', 'gerichtsakten', 'ostwestfalens', 'funston', 'bambergs', 'czapski', 'staatsuniversität', 'vallaster', 'gifsuryvette', 'tuning', 'knolls', 'nauroth', 'zeitungswissenschaft', 'dietenhofen', 'armenisch', 'kunstgewerbemuseums', 'militärwaffen', 'lgötzen', 'kinderdentist', 'hilfst', 'zeigegeste', 'korrektes', 'dervi', 'erolu', 'monogame', 'voglers', 'handelsgeschäfte', 'gefäßformen', 'böhlau', 'quellenfrei', 'ipadressen', 'gnesiolutheraner', 'philippisten', 'zugerechnete', 'lakonischen', 'herzjesuverehrung', 'rennleitung', 'schutzumschlag', 'libris', 'savoiamarchetti', 'südatlantikluftverkehr', 'saintimier', 'pquier', 'gameplays', 'alkoholikers', 'anlegestellen', 'doktorexamen', 'verschaffelt', 'kristallographie']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] <= 1:\n",
    "        train_rare_words.append(word)\n",
    "        \n",
    "print(len(train_rare_words))\n",
    "print(train_rare_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Stem Words: 91121\n",
      "Rare stem Words freq 83273\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "train_rare_stem_words = []\n",
    "\n",
    "for word in train_rare_words:\n",
    "    train_rare_stem_words.append(stemmer.stem(word))\n",
    "\n",
    "print(\"Rare Stem Words:\", len(train_rare_stem_words))\n",
    "\n",
    "train_rare_stem_words_freq = Counter(train_rare_stem_words)\n",
    "\n",
    "print(\"Rare stem Words freq\", len(train_rare_stem_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drannehm', 'marktverstandnis', 'heilbad', 'muttersohnkonflikt', 'alfried', 'abrakadabra', 'jadebus', 'rohstoffknapp', 'privatkraftwag', 'indoar', 'schieb', 'kartenles', 'juniorengrandprixfinal', 'platzanweis', 'abkauft', 'fehlerkorrektur', 'franzoseneinfall', 'knollig', 'obi', 'maissort', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoff', 'stahlfelg', 'tankdeckel', 'gerst', 'bestreicht', 'brotchenhalft', 'ausgebeutet', 'ausbeut', 'morder', 'verhandlungssach', 'luftkraftstoffgem', 'emil', 'gom', 'nassauidenstein', 'logoireih', 'hereinhol', 'dunenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'tauernfenst', 'gerichtsakt', 'funston', 'bamberg', 'czapski', 'staatsuniversitat', 'vallast', 'gifsuryvett', 'tuning', 'knoll', 'nauroth', 'dietenhof', 'armen', 'kunstgewerbemuseum', 'militarwaff', 'lgotz', 'kinderdentist', 'zeigeg', 'korrekt', 'dervi', 'erolu', 'monogam', 'vogl', 'handelsgeschaft', 'bohlau', 'quellenfrei', 'gnesiolutheran', 'philippist', 'zugerechnet', 'herzjesuverehr', 'schutzumschlag', 'libris', 'savoiamarchetti', 'sudatlantikluftverkehr', 'saintimi', 'pquier', 'gameplays', 'anlegestell', 'doktorexam', 'verschaffelt', 'kristallographi', 'verdunkelungsgefahr', 'tonleit', 'zigeunerdurtonleit', 'kontrabassist', 'remich', 'grevenmach', 'poo', 'verbundsicherheitsglas', 'windschutzscheib', 'dnepr', 'vogon', 'einsprach', 'homilet', 'kennedyra']\n"
     ]
    }
   ],
   "source": [
    "train_rare_stem_words_2 = []\n",
    "\n",
    "train_rare_stem_words_freq_keys = train_rare_stem_words_freq.keys()\n",
    "\n",
    "for word in train_rare_stem_words_freq:\n",
    "    if train_rare_stem_words_freq[word] <= 1:\n",
    "        train_rare_stem_words_2.append(word)\n",
    "        \n",
    "print(train_rare_stem_words_2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123944\n",
      "66504\n"
     ]
    }
   ],
   "source": [
    "print(len(train_word_stem_freq))\n",
    "\n",
    "rare_stem_words = []\n",
    "\n",
    "train_word_stem_freq_keys = train_word_stem_freq.keys()\n",
    "\n",
    "for word in train_word_stem_freq:\n",
    "    if train_word_stem_freq[word] < 2:\n",
    "        rare_stem_words.append(word)\n",
    "        \n",
    "print(len(rare_stem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120003\n",
      "65216\n"
     ]
    }
   ],
   "source": [
    "train_word_stem_lemma_freq = Counter(train_words_stemmed_lemma_List)\n",
    "\n",
    "print(len(train_word_stem_lemma_freq))\n",
    "\n",
    "rare_stem_lemma_words = []\n",
    "\n",
    "train_word_stem_freq_keys = train_word_stem_lemma_freq.keys()\n",
    "\n",
    "for word in train_word_stem_lemma_freq:\n",
    "    if train_word_stem_lemma_freq[word] < 2:\n",
    "        rare_stem_lemma_words.append(word)\n",
    "        \n",
    "print(len(rare_stem_lemma_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words = [token.text\n",
    "         for token in val_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val words:  68675\n",
      "Val words dict:  29735\n"
     ]
    }
   ],
   "source": [
    "print(\"Val words: \", len(val_words))\n",
    "val_words_freq = Counter(val_words)\n",
    "print(\"Val words dict: \", len(val_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232\n"
     ]
    }
   ],
   "source": [
    "val_train_set = []\n",
    "for word in train_rare_words:\n",
    "    if val_words_freq[word] >= 1:\n",
    "        val_train_set.append(word)\n",
    "        \n",
    "print(len(val_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [token.text\n",
    "         for token in test_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words:  67545\n",
      "Test words dict:  29553\n"
     ]
    }
   ],
   "source": [
    "print(\"Test words: \", len(test_words))\n",
    "test_words_freq = Counter(test_words)\n",
    "print(\"Test words dict: \", len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3235\n"
     ]
    }
   ],
   "source": [
    "test_train_set = []\n",
    "for word in train_rare_words:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_set.append(word)\n",
    "        \n",
    "print(len(test_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris/checkpoint-51000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
