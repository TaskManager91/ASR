{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "#spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"de_dep_news_trf\")\n",
    "nlp.max_length = 17000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_doc_trf.pickle', 'rb') as handle:\n",
    "    train_doc_trf_loaded = pickle.load(handle)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246525"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_doc_trf_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/val_doc_trf.pickle', 'rb') as handle:\n",
    "     val_doc_trf_loaded = pickle.load(handle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15588"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_doc_trf_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_doc_trf.pickle', 'rb') as handle:\n",
    "    test_doc_trf_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15588"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_doc_trf_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "with open('data/train_tok_text.pickle', 'rb') as handle:\n",
    "    train_tok_text = pickle.load(handle)\n",
    "    \n",
    "with open('data/train_tok_lemma.pickle', 'rb') as handle:\n",
    "    train_tok_lemma = pickle.load(handle)\n",
    "\n",
    "with open('data/train_nouns_text.pickle', 'rb') as handle:\n",
    "    train_nouns_text = pickle.load(handle)\n",
    "\n",
    "with open('data/train_nouns_lemma.pickle', 'rb') as handle:\n",
    "    train_nouns_lemma = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train words:  1138469\n",
      "train words dict:  167707\n"
     ]
    }
   ],
   "source": [
    "print(\"train words: \", len(train_tok_text))\n",
    "train_words_freq = Counter(train_tok_text)\n",
    "print(\"train words dict: \", len(train_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma Noun Words: 1138469\n",
      "Lemma Noun freq Words 151687\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemma Noun Words:\", len(train_tok_lemma))\n",
    "train_tok_lemma_freq = Counter(train_tok_lemma)\n",
    "print(\"Lemma Noun freq Words\", len(train_tok_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n",
      "found!\n"
     ]
    }
   ],
   "source": [
    "for word in train_words:\n",
    "    if(word == \"ausgebeutet\"):\n",
    "        print(\"found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train all:  2350690\n",
      "train words dict:  168237\n",
      "train words:  1138469\n",
      "train words dict:  167707\n",
      "train nouns:  143335\n",
      "train nouns dict:  26890\n"
     ]
    }
   ],
   "source": [
    "print(\"train all: \", len(train_all))\n",
    "train_all_freq = Counter(train_all)\n",
    "print(\"train words dict: \", len(train_all_freq))\n",
    "\n",
    "print(\"train words: \", len(train_words))\n",
    "train_words_freq = Counter(train_words)\n",
    "print(\"train words dict: \", len(train_words_freq))\n",
    "\n",
    "print(\"train nouns: \", len(train_nouns))\n",
    "train_nouns_freq = Counter(train_nouns)\n",
    "print(\"train nouns dict: \", len(train_nouns_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma nouns test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train nouns:  444395\n",
      "train nouns dict:  86470\n"
     ]
    }
   ],
   "source": [
    "print(\"train nouns: \", len(train_nouns_text))\n",
    "train_nouns_freq = Counter(train_nouns_text)\n",
    "print(\"train nouns dict: \", len(train_nouns_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train nouns lemma:  444395\n",
      "train nouns lemma dict:  86145\n"
     ]
    }
   ],
   "source": [
    "print(\"train nouns lemma: \", len(train_nouns_lemma))\n",
    "train_nouns_lemma_freq = Counter(train_nouns_lemma)\n",
    "print(\"train nouns lemma dict: \", len(train_nouns_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare Lemma words 1 :  51127\n",
      "Number of rare Lemma words 2 :  64266\n",
      "Number of rare Lemma words 3 :  70137\n",
      "Number of rare Lemma words 4 :  73453\n",
      "Number of rare Lemma words 5 :  75584\n",
      "Number of rare Lemma words 6 :  77073\n",
      "Number of rare Lemma words 7 :  78164\n",
      "Number of rare Lemma words 8 :  78991\n",
      "Number of rare Lemma words 9 :  79663\n",
      "Number of rare Lemma words 10 :  80238\n"
     ]
    }
   ],
   "source": [
    "train_nouns_lemma_freq_keys = train_nouns_lemma_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(1,n):\n",
    "    \n",
    "    rare_lemma_words = []\n",
    "    for word in train_nouns_lemma_freq_keys:\n",
    "        \n",
    "        if train_nouns_lemma_freq[word] <= i:\n",
    "            rare_lemma_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare Lemma words \" + str(i) + \" : \", len(rare_lemma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['schiebers', 'sturmes', 'piscina', 'tankdeckels', 'fertigungsanlage', 'kopie', 'tauernfensters', 'staatsuniversität', 'modellautos', 'hufen', 'rennleitung', 'schutzumschlag', 'highschoolzeit', 'kontrabassisten', 'homiletik', 'heiliggeistordens', 'beifall', 'geschichtspolitik', 'instandsetzungstruppe', 'merowingern', 'austreiben', 'sesklokultur', 'ovale', 'trajektorie', 'umsatzbeteiligung', 'gesellschafterversammlung', 'wood', 'trialog', 'granada', 'bildpunkt', 'einsatzfall', 'zarismus', 'feudalismus', 'realschulempfehlung', 'boxsack', 'versuchsanordnung', 'christin', 'xte', 'matheprüfung', 'weihnachtsbaumdekoration', 'modelltheorie', 'böhme', 'unterkieferknochen', 'ormandy', 'cholerakonferenz', 'arbeitsbewertung', 'handymusik', 'komponist', 'audiodaten', 'barchfeld', 'antonio', 'adour', 'humorvoll', 'master', 'freileitung', 'gesundheitsschutzniveau', 'ausgabenprogramm', 'schiffsunglück', 'friedenszeiten', 'mennonitengemeinde', 'leitungsfunktion', 'tritt', 'sauna', 'stiftungsprofessur', 'fernheizwerk', 'mikrofossilien', 'altenhof', 'adsorption', 'karmels', 'lebensgewohnheiten', 'passunion', 'sitzheizung', 'lombardei', 'debüts', 'südsüdhandels', 'preistheorie', 'gebäuderückseite', 'beobachter', 'lagersystem', 'babyklappe', 'felsenlandjugendherberge', 'jugendherbergswerks', 'klopapier', 'zerrung', 'fünfinfünfauspuffanlage', 'geräuschentwicklung', 'hammerschlag', 'beschleunigen', 'landtagsausschuss', 'taiwaner', 'bruchsteinsockel', 'gehhilfe', 'kreisversammlung', 'städtchens', 'lehrbuch']\n"
     ]
    }
   ],
   "source": [
    "train_rare_lemma_words = []\n",
    "for word in train_nouns_lemma_freq_keys:\n",
    "    if train_nouns_lemma_freq[word] <= 1:\n",
    "        train_rare_lemma_words.append(word)\n",
    "\n",
    "print(train_rare_lemma_words[5:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemma words test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma Words: 1138469\n",
      "Lemma freq Words 151687\n"
     ]
    }
   ],
   "source": [
    "train_words_lemma = [token.lemma_\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct)]\n",
    "\n",
    "print(\"Lemma Words:\", len(train_words_lemma))\n",
    "train_words_lemma_freq = Counter(train_words_lemma)\n",
    "print(\"Lemma freq Words\", len(train_words_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found!\n",
      "found!\n",
      "found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in train_words:\n",
    "    if word == \"rutschte\":\n",
    "        print(\"found!\")\n",
    "\n",
    "train_words_freq['rutschte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare Lemma words 1 :  84290\n",
      "Number of rare Lemma words 2 :  108559\n",
      "Number of rare Lemma words 3 :  119610\n",
      "Number of rare Lemma words 4 :  125815\n",
      "Number of rare Lemma words 5 :  129823\n",
      "Number of rare Lemma words 6 :  132562\n",
      "Number of rare Lemma words 7 :  134655\n",
      "Number of rare Lemma words 8 :  136284\n",
      "Number of rare Lemma words 9 :  137614\n",
      "Number of rare Lemma words 10 :  138702\n"
     ]
    }
   ],
   "source": [
    "train_words_lemma_freq_keys = train_words_lemma_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(1,n):\n",
    "    \n",
    "    train_rare_words = []\n",
    "    for word in train_words_lemma_freq_keys:\n",
    "        \n",
    "        if train_words_lemma_freq[word] <= i:\n",
    "            train_rare_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare Lemma words \" + str(i) + \" : \", len(train_rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alfried', 'abrakadabra', 'abdrücken', 'jadebusen', 'rohstoffknappheit', 'privatkraftwagen', 'indoarischen', 'schiebers', 'kartenleser', 'juniorengrandprixfinale', 'platzanweiser', 'sturmes', 'abkauft', 'fehlerkorrekturen', 'franzoseneinfall', 'knollig', 'obie', 'maissorten', 'piscina', 'landtagswahlkreis', 'turatta', 'mitanni', 'lscheich', 'geldkoffer', 'stahlfelgen', 'rutschen', 'tankdeckels', 'gerst', 'bestreicht', 'brötchenhälfte', 'ausgebeuteten', 'ausbeuter', 'mörderisch', 'verhandlungssache', 'luftkraftstoffgemisch', 'fertigungsanlage', 'emile', 'gomer', 'nassauidenstein', 'logoireihe', 'hereinholen', 'dünenstinkmorchel', 'bellman', 'cereus', 'tauernbach', 'schiefers', 'tauernfensters', 'gerichtsakten', 'ostwestfalens', 'funston', 'bambergs', 'czapski', 'staatsuniversität', 'vallaster', 'gifsuryvette', 'tuning', 'knolls', 'nauroth', 'zeitungswissenschaft', 'dietenhofen', 'armenisch', 'kunstgewerbemuseums', 'militärwaffen', 'lgötzen', 'kinderdentist', 'hilfst', 'zeigegeste', 'korrektes', 'dervi', 'erolu', 'monogame', 'voglers', 'handelsgeschäfte', 'gefäßformen', 'böhlau', 'quellenfrei', 'ipadressen', 'gnesiolutheraner', 'philippisten', 'zugerechnete', 'lakonischen', 'herzjesuverehrung', 'rennleitung', 'schutzumschlag', 'libris', 'savoiamarchetti', 'südatlantikluftverkehr', 'saintimier', 'pquier', 'gameplays', 'alkoholikers', 'anlegestellen', 'doktorexamen', 'verschaffelt', 'kristallographie']\n"
     ]
    }
   ],
   "source": [
    "train_rare_words = []\n",
    "for word in train_words_freq_keys:\n",
    "    if train_words_freq[word] <= 1:\n",
    "        train_rare_words.append(word)\n",
    "\n",
    "print(train_rare_words[5:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rare words 0 :  0\n",
      "Number of rare words 1 :  91121\n",
      "Number of rare words 2 :  118265\n",
      "Number of rare words 3 :  130694\n",
      "Number of rare words 4 :  137805\n",
      "Number of rare words 5 :  142409\n",
      "Number of rare words 6 :  145643\n",
      "Number of rare words 7 :  148076\n",
      "Number of rare words 8 :  149990\n",
      "Number of rare words 9 :  151534\n",
      "Number of rare words 10 :  152818\n"
     ]
    }
   ],
   "source": [
    "train_words_freq_keys = train_words_freq.keys()\n",
    "\n",
    "n = 11\n",
    "\n",
    "for i in range(0,n):\n",
    "    \n",
    "    rare_words = []\n",
    "    for word in train_words_freq_keys:\n",
    "        \n",
    "        if train_words_freq[word] <= i:\n",
    "            rare_words.append(word)\n",
    "        \n",
    "    print(\"Number of rare words \" + str(i) + \" : \", len(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91121\n",
      "118265\n",
      "130694\n",
      "137805\n",
      "142409\n",
      "145643\n",
      "148076\n",
      "149990\n",
      "151534\n",
      "152818\n"
     ]
    }
   ],
   "source": [
    "train_rare_words = {}\n",
    "\n",
    "for i in range(1,11):\n",
    "    train_rare_words[i] = []\n",
    "    for word in train_words_freq_keys:\n",
    "        if train_words_freq[word] <= i:\n",
    "            train_rare_words[i].append(word)\n",
    "        \n",
    "    print(len(train_rare_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Stem Words: 91121\n",
      "Rare stem Words freq 83273\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "train_rare_stem_words = []\n",
    "\n",
    "for word in train_rare_words:\n",
    "    train_rare_stem_words.append(stemmer.stem(word))\n",
    "\n",
    "print(\"Rare Stem Words:\", len(train_rare_stem_words))\n",
    "\n",
    "train_rare_stem_words_freq = Counter(train_rare_stem_words)\n",
    "\n",
    "print(\"Rare stem Words freq\", len(train_rare_stem_words_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words = [token.text\n",
    "         for token in val_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words_trf = [token.text\n",
    "         for token in val_doc_trf\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(val_words == val_words_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lemma = [token.lemma_\n",
    "         for token in val_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val lemma:  68675\n",
      "Val words dict:  26350\n"
     ]
    }
   ],
   "source": [
    "print(\"Val lemma: \", len(val_lemma))\n",
    "val_lemma_freq = Counter(val_lemma)\n",
    "print(\"Val words dict: \", len(val_lemma_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18099\n",
      "['feick', 'geflügel', 'schweinezucht', 'beschäftigungen', 'okci', 'fischingen', 'westsahara', 'kisten', 'protein', 'lebenswichtig', 'kostüm', 'hingucker', 'frühchen', 'postkarte', 'christiane', 'matheprüfung', 'mithelfen', 'maisfeldes', 'labyrinth', 'aufzubocken', 'tschüss', 'streunend', 'kater', 'massenweise', 'spatzen', 'frostschutzmittel', 'schnupfen', 'krukenbergspindel', 'saintdenis', 'kaspar', 'wolfersweiler', 'unfalls', 'schlackerst', 'merlin', 'fleisch', 'fleischwolf', 'aufgeditscht', 'haushaltsbeschränkung', 'kelsos', 'harfe', 'auffassungen', 'rauschgifthandels', 'stabhochsprung', 'enttäuschung', 'doppelkabine', 'burmanniaarten', 'daddeln', 'schwielen', 'headliner', 'saarlouis', 'abschnittweise', 'apotheker', 'geschlechtsdimorphismus', 'frühstückt', 'personalleiter', 'ausnahmetalent', 'unzüchtiges', 'krokodilklemmen', 'ankurbeln', 'ladenbesitzer', 'schiebetür', 'zahnlücke', 'ausflippen', 'erwidern', 'olga', 'ernten', 'säen', 'umsiedeln', 'fix', 'konstante', 'landbesitz', 'werkzeugen', 'ersatzteilen', 'nahrungsmangel', 'marienkäferlarven', 'wanderungsbilanz', 'köpke', 'eintracht', 'ramona', 'kali', 'liiert', 'tenplatz', 'primitiv', 'vielzeller', 'klempner', 'takoradi', 'palais', 'steinmaur', 'römern', 'fortbewegung', 'paddeln', 'bildungssprache', 'estlands', 'liturgisch', 'agrarwissenschaften', 'diplomlandwirt', 'obdachlosenasyl', 'gemeindegebiets', 'langjökull', 'wohnanschrift']\n"
     ]
    }
   ],
   "source": [
    "rare_val_lemma = []\n",
    "\n",
    "val_lemma_freq_keys = val_lemma_freq.keys()\n",
    "\n",
    "for word in val_lemma_freq_keys:\n",
    "    if val_lemma_freq[word] <= 1:\n",
    "        rare_val_lemma.append(word)\n",
    "\n",
    "print(len(rare_val_lemma))\n",
    "print(rare_val_lemma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val words:  68675\n",
      "Val words dict:  29735\n"
     ]
    }
   ],
   "source": [
    "print(\"Val words: \", len(val_words))\n",
    "val_words_freq = Counter(val_words)\n",
    "print(\"Val words dict: \", len(val_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20551\n",
      "['feick', 'geflügel', 'schweinezucht', 'beschäftigungen', 'okci', 'fischingen', 'westsahara', 'kisten', 'stopfen', 'protein', 'lebenswichtig', 'kostüm', 'hingucker', 'frühchen', 'errechneten', 'postkarte', 'christiane', 'matheprüfung', 'mitgeholfen', 'maisfeldes', 'labyrinth', 'aufzubocken', 'tschüss', 'streunender', 'kater', 'massenweise', 'spatzen', 'natürlichem', 'frostschutzmittel', 'harmloser', 'schnupfen', 'krukenbergspindel', 'saintdenis', 'kaspar', 'wolfersweiler', 'unfalls', 'abfällt', 'schlackerst', 'merlin', 'fleisch', 'fleischwolf', 'versunken', 'aufgeditscht', 'haushaltsbeschränkung', 'einzubinden', 'kelsos', 'fremde', 'harfe', 'auffassungen', 'rauschgifthandels', 'stabhochsprung', 'enttäuschung', 'doppelkabine', 'burmanniaarten', 'daddeln', 'schwielen', 'headliner', 'saarlouis', 'abschnittweise', 'apotheker', 'ausgeprägter', 'geschlechtsdimorphismus', 'frühstückt', 'wundern', 'personalleiter', 'ausnahmetalent', 'unzüchtiges', 'krokodilklemmen', 'angekurbelt', 'ladenbesitzer', 'merkte', 'freuten', 'schiebetür', 'einklemmte', 'zahnlücke', 'auszuflippen', 'erwiderte', 'olga', 'erntet', 'sät', 'umliegende', 'umgesiedelt', 'fixe', 'konstante', 'landbesitz', 'werkzeugen', 'ersatzteilen', 'nahrungsmangel', 'marienkäferlarven', 'wanderungsbilanz', 'köpke', 'eintracht', 'ramona', 'kali', 'liiert', 'tenplatz', 'primitivsten', 'vielzeller', 'klempner', 'verharmlosen']\n"
     ]
    }
   ],
   "source": [
    "rare_val_words = []\n",
    "\n",
    "val_words_freq_keys = val_words_freq.keys()\n",
    "\n",
    "for word in val_words_freq_keys:\n",
    "    if val_words_freq[word] <= 1:\n",
    "        rare_val_words.append(word)\n",
    "\n",
    "print(len(rare_val_words))\n",
    "print(rare_val_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232\n"
     ]
    }
   ],
   "source": [
    "val_train_set = []\n",
    "for word in train_rare_words:\n",
    "    if val_words_freq[word] >= 1:\n",
    "        val_train_set.append(word)\n",
    "        \n",
    "print(len(val_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15588/15588 [00:16<00:00, 973.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tok_text = []\n",
    "for i in tqdm(range(len(test_doc_trf_loaded))):\n",
    "    test_tok_text = test_tok_text + [token.text for token in test_doc_trf_loaded[i] if(not token.is_stop and not token.is_punct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words:  67545\n",
      "Test words dict:  29553\n"
     ]
    }
   ],
   "source": [
    "print(\"Test words: \", len(test_tok_text))\n",
    "test_words_freq = Counter(test_tok_text)\n",
    "print(\"Test words dict: \", len(test_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_freq['arbeitsunfähigkeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n"
     ]
    }
   ],
   "source": [
    "test_words_freq_keys = test_words_freq.keys()\n",
    "\n",
    "test_only_words = []\n",
    "\n",
    "for word in test_words_freq_keys:\n",
    "    if train_words_freq[word] == 0:\n",
    "        test_only_words.append(word)\n",
    "\n",
    "print(len(test_only_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arbeitsunfähig\n"
     ]
    }
   ],
   "source": [
    "print(test_only_words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [token.text\n",
    "         for token in test_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all:  140713\n",
      "Test all dict:  30042\n"
     ]
    }
   ],
   "source": [
    "test_all = [token.text\n",
    "         for token in test_doc\n",
    "         if not token.is_punct]\n",
    "\n",
    "print(\"Test all: \", len(test_all))\n",
    "test_all_freq = Counter(test_all)\n",
    "print(\"Test all dict: \", len(test_all_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3235\n",
      "3418\n"
     ]
    }
   ],
   "source": [
    "test_train_set = []\n",
    "test_train_count = 0\n",
    "for word in train_rare_words:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count = test_train_count + test_words_freq[word]\n",
    "        test_train_set.append(word)\n",
    "        \n",
    "print(len(test_train_set))\n",
    "print(test_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239\n",
      "2446\n"
     ]
    }
   ],
   "source": [
    "test_train_set_2 = []\n",
    "test_train_count_2 = 0\n",
    "for word in train_rare_words_2:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_2 = test_train_count_2 + test_words_freq[word]\n",
    "        test_train_set_2.append(word)\n",
    "        \n",
    "print(len(test_train_set_2))\n",
    "print(test_train_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n",
      "1758\n"
     ]
    }
   ],
   "source": [
    "test_train_set_3 = []\n",
    "test_train_count_3 = 0\n",
    "for word in train_rare_words_3:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_3 = test_train_count_3 + test_words_freq[word]\n",
    "        test_train_set_3.append(word)\n",
    "        \n",
    "print(len(test_train_set_3))\n",
    "print(test_train_count_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drüben\n",
      "eingezeichnet\n",
      "packung\n",
      "1280\n",
      "1486\n"
     ]
    }
   ],
   "source": [
    "test_train_set_4 = []\n",
    "test_train_count_4 = 0\n",
    "for word in train_rare_words_4:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_4 = test_train_count_4 + test_words_freq[word]\n",
    "        test_train_set_4.append(word)\n",
    "    if test_words_freq[word] >= 4:\n",
    "        print(word)\n",
    "        \n",
    "print(len(test_train_set_4))\n",
    "print(test_train_count_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "test_train_set_5 = []\n",
    "test_train_count_5 = 0\n",
    "for word in train_rare_words_5:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_5 = test_train_count_5 + test_words_freq[word]\n",
    "        test_train_set_5.append(word)\n",
    "        \n",
    "print(len(test_train_set_5))\n",
    "print(test_train_count_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bremerhaven\n",
      "sahne\n",
      "enzyme\n",
      "geste\n",
      "regierungssitz\n",
      "bewaffnet\n",
      "teppich\n",
      "erwachsen\n",
      "versteckte\n",
      "segen\n",
      "isst\n",
      "erzählte\n",
      "verbraucht\n",
      "pole\n",
      "warmen\n",
      "angetroffen\n",
      "549\n",
      "808\n"
     ]
    }
   ],
   "source": [
    "test_train_set_10 = []\n",
    "test_train_count_10 = 0\n",
    "for word in train_rare_words_10:\n",
    "    if test_words_freq[word] >= 1:\n",
    "        test_train_count_10 = test_train_count_10 + test_words_freq[word]\n",
    "        test_train_set_10.append(word)\n",
    "    if test_words_freq[word] >= 4:\n",
    "        print(word)\n",
    "        \n",
    "print(len(test_train_set_10))\n",
    "print(test_train_count_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris/checkpoint-51000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    input_values = processor(\n",
    "          batch[\"speech\"], \n",
    "          sampling_rate=batch[\"sampling_rate\"], \n",
    "          return_tensors=\"pt\"\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "test_sampled = load_from_disk(\"/media/chris/TheFlash/Master/data/test_sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_sampled.map(map_to_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.save_to_disk(\"results51_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "results = load_from_disk(\"results51_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.147\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "results = load_from_disk(\"results51_cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15588/15588 [07:11<00:00, 36.11it/s]\n"
     ]
    }
   ],
   "source": [
    "result_complete_text = \"\"\n",
    "for i in tqdm(range(results.shape[0])):\n",
    "    result_complete_text = result_complete_text + results[i]['pred_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc = nlp(result_complete_text, disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc_trf = nlp(result_complete_text, disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████████▍                                    | 8191/15588 [07:51<07:33, 16.32it/s]"
     ]
    }
   ],
   "source": [
    "result_doc_trf = []\n",
    "for i in tqdm(range(results.shape[0])):\n",
    "    result_doc_trf.append(nlp(results[i]['pred_str'], disable = ['ner', 'parser']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result_doc_trf, open(\"data/result_doc_trf_cut.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(result_doc_trf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_doc_trf.pickle', 'rb') as handle:\n",
    "    result_doc_trf_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15588/15588 [00:02<00:00, 6663.36it/s]\n"
     ]
    }
   ],
   "source": [
    "result_tok_text = []\n",
    "for i in tqdm(range(len(result_doc_trf))):\n",
    "    result_tok_text = result_tok_text + [token.text for token in result_doc_trf[i] if(not token.is_stop and not token.is_punct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result words:  66319\n",
      "Result words dict:  40662\n"
     ]
    }
   ],
   "source": [
    "result_words = [token.text\n",
    "         for token in result_doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result words:  67814\n",
      "Result words dict:  33773\n"
     ]
    }
   ],
   "source": [
    "print(\"Result words: \", len(result_tok_text))\n",
    "result_words_freq = Counter(result_tok_text)\n",
    "print(\"Result words dict: \", len(result_words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15588/15588 [00:03<00:00, 4499.17it/s]\n"
     ]
    }
   ],
   "source": [
    "result_all = []\n",
    "for i in tqdm(range(len(result_doc_trf_loaded))):\n",
    "    result_all = result_all + [token.text for token in result_doc_trf_loaded[i] if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result words:  123501\n",
      "Result words dict:  41131\n"
     ]
    }
   ],
   "source": [
    "result_all = [token.text\n",
    "         for token in result_doc\n",
    "         if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result words:  139088\n",
      "Result words dict:  34255\n"
     ]
    }
   ],
   "source": [
    "print(\"Result words: \", len(result_all))\n",
    "result_all_freq = Counter(result_all)\n",
    "print(\"Result words dict: \", len(result_all_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5810\n",
      "2782\n",
      "0.47882960413080894\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_only_words))\n",
    "for word in test_only_words:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_only_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words dict:  29553\n",
      "21063\n"
     ]
    }
   ],
   "source": [
    "print(\"Test words dict: \", len(test_words_freq))\n",
    "\n",
    "test_words_freq_keys = test_words_freq.keys()\n",
    "\n",
    "i = 0\n",
    "for word in test_words_freq_keys:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_train = pickle.load(open(\"data/index_test_train.p\", \"rb\"))\n",
    "importantLists = [1,2,3,4,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3235\n",
      "1849\n",
      "0.5715610510046368\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_train_set))\n",
    "for word in test_train_set:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_train_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239\n",
      "1407\n",
      "0.6284055381866905\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_train_set_2))\n",
    "for word in test_train_set_2:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_train_set_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n",
      "993\n",
      "0.6456436931079323\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_train_set_3))\n",
    "for word in test_train_set_3:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_train_set_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "842\n",
      "0.6578125\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_train_set_4))\n",
    "for word in test_train_set_4:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_train_set_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063\n",
      "718\n",
      "0.6754468485418627\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_train_set_5))\n",
    "for word in test_train_set_5:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_train_set_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "428\n",
      "0.7795992714025501\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(len(test_train_set_10))\n",
    "for word in test_train_set_10:\n",
    "    if result_words_freq[word] >= 1:\n",
    "        i = i+1\n",
    "print(i)\n",
    "\n",
    "print((i/len(test_train_set_10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
