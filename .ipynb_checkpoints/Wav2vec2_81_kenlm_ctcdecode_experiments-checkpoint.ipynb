{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kenlm\n",
    "import ctcdecode\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, load_metric, Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1gram\n",
      "found 2gram\n"
     ]
    }
   ],
   "source": [
    "alpha = 2.5 # LM Weight\n",
    "beta = 0.0 # LM Usage Reward\n",
    "vocab = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_']\n",
    "\n",
    "word_lm_scorer = ctcdecode.WordKenLMScorer('lm_data/train.arpa', alpha, beta) # use your own kenlm model\n",
    "\n",
    "decoder = ctcdecode.BeamSearchDecoder(\n",
    "    vocab,\n",
    "    num_workers=2,\n",
    "    beam_width=128,\n",
    "    scorers=[word_lm_scorer],\n",
    "    cutoff_prob=np.log(0.000001),\n",
    "    cutoff_top_n=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix(\"\", 0.0, \"None\", 0.0, -inf)\n"
     ]
    }
   ],
   "source": [
    "from ctcdecode.prefix import State\n",
    "\n",
    "# Initialize prefixes\n",
    "prefixes = State(\n",
    "    scorers=[word_lm_scorer],\n",
    "    size=128\n",
    ")\n",
    "\n",
    "for prefix in prefixes:\n",
    "    print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res51_full_log = load_from_disk(\"lm_full_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "92\n",
      "ihre fotostrecken erschienen in modemagazinen wie der vogue harpers bazaar und marie claire \n"
     ]
    }
   ],
   "source": [
    "logs = res51_full_log[2]['lm_raw']\n",
    "text = res51_full_log[2]['lm_str']\n",
    "text = res51_full_log[2]['target_text']\n",
    "print(len(logs))\n",
    "print(len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "logs = np.asarray(logs)\n",
    "nT = logs.shape[0]\n",
    "print(nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruned_vocab_indices(log_probs):\n",
    "    \n",
    "    cutoff_prob=np.log(0.000001)\n",
    "    cutoff_top_n=40\n",
    "    \n",
    "    \"\"\" Return vocab indices of pruned probabilities of a time step. \"\"\"\n",
    "\n",
    "    index_to_prob = [(k, log_probs[k]) for k in range(log_probs.shape[0])]\n",
    "    index_to_prob = sorted(index_to_prob, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if cutoff_top_n < len(index_to_prob):\n",
    "        index_to_prob = index_to_prob[:cutoff_top_n]\n",
    "\n",
    "    if cutoff_prob < 1.0:\n",
    "        filtered = []\n",
    "        for x in index_to_prob:\n",
    "            if x[1] >= cutoff_prob:\n",
    "                filtered.append(x)\n",
    "        index_to_prob = filtered\n",
    "\n",
    "    return [x[0] for x in index_to_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "\n",
      "33\n",
      "[ -3.14541698  -3.76524782  -6.66179562  -5.16091967  -3.92799187\n",
      "  -4.91117573  -2.58480668  -4.42430305   0.25773394  -5.30773211\n",
      "  -1.49122858  -2.32450509  -4.87767696  -5.76897573  -2.33950615\n",
      "  -3.94859457  -2.17412162  -3.89775801  -2.74408746  -1.11488378\n",
      "  -5.61254215  -2.56076002  -2.87019825  -3.36453319  -6.88686419\n",
      "  -6.19530964  -0.64040613  -3.38129091  -6.03537321  -3.85242057\n",
      "  -3.87596941 -17.70882416  11.8444109 ]\n",
      "32\n",
      "[32, 8, 26, 19, 10, 16, 11, 14, 21, 6, 18, 22, 0, 23, 27, 1, 29, 30, 17, 4, 15, 7, 12, 5, 3, 9, 20, 13, 28, 25, 2, 24]\n",
      "\n",
      "33\n",
      "[ -3.36899519  -4.43510294  -6.61256313  -6.11390257  -4.59750032\n",
      "  -4.83982563  -3.82120395  -5.33151674  -1.2708199   -5.74136782\n",
      "  -1.47957909  -3.00136232  -4.17404079  -6.08344412  -2.56120777\n",
      "  -4.98186874  -2.53568602  -3.56372023  -3.00162482  -1.67863286\n",
      "  -6.66186666  -3.39269567  -3.80653071  -3.3572135   -7.5221324\n",
      "  -6.34197712  -1.40185916  -3.97075295  -6.73809624  -3.15032697\n",
      "  -4.36318111 -19.58986282  13.28065872]\n",
      "32\n",
      "[32, 8, 26, 10, 19, 16, 14, 11, 18, 29, 23, 0, 21, 17, 22, 6, 27, 12, 30, 1, 4, 5, 15, 7, 9, 13, 3, 25, 2, 20, 28, 24]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "for t in range(2):\n",
    "    print(\"\")\n",
    "    step_probs = logs[t]\n",
    "    print(len(step_probs))\n",
    "    print(step_probs)\n",
    "    pruned_step_probs = get_pruned_vocab_indices(step_probs)\n",
    "    print(len(pruned_step_probs))\n",
    "    print(pruned_step_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________________i_h__ree_ __f__o_r__t__e____s_tr_e_c____ke__n  __e_r_sscch_i_ee_nn_en_  mi___t_ _m_o_____le____m_a____g__a______z__i____nne_ _w_ie_ d_e_r  _w____o______g___ __________a___t_ _d_a__s__ _b__a_____s_a_____r______w__a____r__y_______c__l_a____r__________________________________________________ \n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for t in range(nT):\n",
    "    step_probs = logs[t]\n",
    "    max_index = step_probs.argmax()\n",
    "    symbol = vocab[max_index]\n",
    "    result = result + symbol\n",
    "\n",
    "print(result)\n",
    "    #pruned_step_probs = get_pruned_vocab_indices(step_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
