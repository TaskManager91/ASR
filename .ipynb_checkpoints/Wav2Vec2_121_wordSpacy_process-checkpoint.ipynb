{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/82/04e9aaf603fdbaecb4323b9e723f13c92c245f6ab2902195c53987848c78/pip-21.1.2-py3-none-any.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 10.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting setuptools\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/78/56aa1b5f4d8ac548755ae767d84f0be54fdd9d404197a3d9e4659d272348/setuptools-57.0.0-py3-none-any.whl (821kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 8.1MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting wheel\n",
      "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "Installing collected packages: pip, setuptools, wheel\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "  Found existing installation: setuptools 40.2.0\n",
      "    Uninstalling setuptools-40.2.0:\n",
      "      Successfully uninstalled setuptools-40.2.0\n",
      "  Found existing installation: wheel 0.31.1\n",
      "    Uninstalling wheel-0.31.1:\n",
      "      Successfully uninstalled wheel-0.31.1\n",
      "Successfully installed pip-21.1.2 setuptools-57.0.0 wheel-0.36.2\n",
      "Collecting spacy[cuda112]\n",
      "  Downloading spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
      "\u001b[K     |████████████████████████████████| 456 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading catalogue-2.0.4-py3-none-any.whl (16 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.61.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 5.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy[cuda112]) (2.10)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.5.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading spacy_legacy-3.0.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.3\n",
      "  Downloading thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy[cuda112]) (2.19.1)\n",
      "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy[cuda112]) (57.0.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy[cuda112]) (1.15.1)\n",
      "Collecting cupy-cuda112<9.0.0,>=5.0.0b4\n",
      "  Downloading cupy_cuda112-8.6.0-cp37-cp37m-manylinux1_x86_64.whl (165.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.2 MB 11.2 MB/s eta 0:00:01    |████████                        | 41.3 MB 13.3 MB/s eta 0:00:10\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "Collecting fastrlock>=0.3\n",
      "  Downloading fastrlock-0.6-cp37-cp37m-manylinux1_x86_64.whl (39 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/chris/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->spacy[cuda112]) (2.2.0)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (1.23)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /home/chris/anaconda3/lib/python3.7/site-packages (from jinja2->spacy[cuda112]) (1.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107106 sha256=a4da8943c48a3fbc7eddf2565737b091ea3c7a3ccc69cf2b9b429fbe6209b76a\n",
      "  Stored in directory: /home/chris/.cache/pip/wheels/83/a6/12/bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built smart-open\n",
      "Installing collected packages: zipp, typing-extensions, murmurhash, cymem, click, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, tqdm, thinc, spacy-legacy, pathy, packaging, fastrlock, spacy, cupy-cuda112\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 6.7\n",
      "    Uninstalling click-6.7:\n",
      "      Successfully uninstalled click-6.7\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.26.0\n",
      "    Uninstalling tqdm-4.26.0:\n",
      "      Successfully uninstalled tqdm-4.26.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 17.1\n",
      "    Uninstalling packaging-17.1:\n",
      "      Successfully uninstalled packaging-17.1\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.4 click-7.1.2 cupy-cuda112-8.6.0 cymem-2.0.5 fastrlock-0.6 murmurhash-1.0.5 packaging-20.9 pathy-0.5.2 preshed-3.0.5 pydantic-1.7.4 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 tqdm-4.61.0 typer-0.3.2 typing-extensions-3.10.0.0 wasabi-0.8.2 zipp-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy[cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-dep-news-trf==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_dep_news_trf-3.0.0/de_dep_news_trf-3.0.0-py3-none-any.whl (413.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 413.0 MB 75 kB/s s eta 0:00:01     |███████████████████▌            | 251.1 MB 13.0 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting spacy-transformers<1.1.0,>=1.0.0rc4\n",
      "  Downloading spacy_transformers-1.0.2-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from de-dep-news-trf==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (57.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (1.15.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (3.10.0.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (20.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (4.61.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/chris/anaconda3/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/chris/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.2.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/chris/anaconda3/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chris/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-dep-news-trf==3.0.0) (2018.8.24)\n",
      "Collecting torch>=1.5.0\n",
      "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[K     |█████████████████▌              | 439.3 MB 17.1 MB/s eta 0:00:22    |█▍                              | 33.6 MB 13.3 MB/s eta 0:00:58"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy[transformers,cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "\n",
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "common_voice_train = load_dataset(\"common_voice\", \"de\", split=\"train\", cache_dir=\"D:\\Master\\wsl\\data\")\n",
    "common_voice_validation = load_dataset(\"common_voice\", \"de\", split=\"validation\", cache_dir=\"D:\\Master\\wsl\\data\")\n",
    "common_voice_test = load_dataset(\"common_voice\", \"de\", split=\"test\", cache_dir=\"D:\\Master\\wsl\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_validation = common_voice_validation.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"path\",\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_keep = '[^A-Za-zäüöß ]+'\n",
    "\n",
    "def remove_special_characters_chris(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_keep, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train_text = common_voice_train.map(remove_special_characters_chris)\n",
    "common_voice_validation_text = common_voice_validation.map(remove_special_characters_chris)\n",
    "common_voice_test_text = common_voice_test.map(remove_special_characters_chris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train_text.save_to_disk(\"\")\n",
    "common_voice_validation_text.save_to_disk(\"\")\n",
    "common_voice_test_text.save_to_disk(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "train_text_dataset = load_from_disk(\"E:/Master/data/0_text/train_text\")\n",
    "val_text_dataset = load_from_disk(\"E:/Master/data/0_text/val_text\")\n",
    "test_text_dataset = load_from_disk(\"E:/Master/data/0_text/test_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "train_text_dataset = load_from_disk(\"/media/chris/TheFlash/Master/data/0_text/train_text\")\n",
    "val_text_dataset = load_from_disk(\"/media/chris/TheFlash/Master/data/0_text/val_text\")\n",
    "test_text_dataset = load_from_disk(\"/media/chris/TheFlash/Master/data/0_text/test_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Datasets with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process train @ \", now)\n",
    "\n",
    "train_doc_trf = []\n",
    "for i in tqdm(range(train_text_dataset.shape[0])):\n",
    "    train_doc_trf.append(nlp(train_text_dataset[i]['sentence'], disable = ['ner', 'parser']))\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process train @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_doc_trf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process val @ \", now)\n",
    "\n",
    "val_doc_trf = []\n",
    "for i in tqdm(range(val_text_dataset.shape[0])):\n",
    "    val_doc_trf.append(nlp(val_text_dataset[i]['sentence'], disable = ['ner', 'parser']))\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process val @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('val_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_doc_trf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process test @ \", now)\n",
    "\n",
    "test_doc_trf = []\n",
    "for i in tqdm(range(test_text_dataset.shape[0])):\n",
    "    test_doc_trf.append(nlp(test_text_dataset[i]['sentence'], disable = ['ner', 'parser']))\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process test @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('test_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_doc_trf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_doc_trf.pickle', 'rb') as handle:\n",
    "    train_doc_trf_loaded = pickle.load(handle)\n",
    "    \n",
    "\n",
    "with open('val_doc_trf.pickle', 'rb') as handle:\n",
    "    val_doc_trf_loaded = pickle.load(handle)\n",
    "\n",
    "with open('test_doc_trf.pickle', 'rb') as handle:\n",
    "    test_doc_trf_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Train dataset (text/lemma noun/lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 8\n",
    "train_tok_text_splits = {}\n",
    "for i in range(splits):\n",
    "    train_tok_text_splits[i] = []\n",
    "    \n",
    "splittet_train_trf = np.array_split(train_doc_trf_loaded, splits)\n",
    "\n",
    "for n, token_list in enumerate(splittet_train_trf):\n",
    "    for i in tqdm(range(len(token_list))):\n",
    "        train_tok_text_splits[n] = train_tok_text_splits[n] + [token.text for token in token_list[i] if(not token.is_stop and not token.is_punct)]\n",
    "        \n",
    "train_tok_text = train_tok_text_splits[0]\n",
    "\n",
    "for i in range(1,splits):\n",
    "    train_tok_text = train_tok_text + train_tok_text_splits[i]\n",
    "\n",
    "with open('train_tok_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tok_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 8\n",
    "train_tok_lemma_splits = {}\n",
    "for i in range(splits):\n",
    "    train_tok_lemma_splits[i] = []\n",
    "    \n",
    "splittet_train_trf = np.array_split(train_doc_trf_loaded, splits)\n",
    "\n",
    "for n, token_list in enumerate(splittet_train_trf):\n",
    "    for i in tqdm(range(len(token_list))):\n",
    "        train_tok_lemma_splits[n] = train_tok_lemma_splits[n] + [token.lemma_ for token in token_list[i] if(not token.is_stop and not token.is_punct)]\n",
    "        \n",
    "train_tok_lemma = train_tok_lemma_splits[0]\n",
    "\n",
    "for i in range(1,splits):\n",
    "    train_tok_lemma = train_tok_lemma + train_tok_lemma_splits[i]\n",
    "\n",
    "with open('train_tok_lemma.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tok_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 8\n",
    "train_noun_text_splits = {}\n",
    "for i in range(splits):\n",
    "    train_noun_text_splits[i] = []\n",
    "    \n",
    "splittet_train_trf = np.array_split(train_doc_trf_loaded, splits)\n",
    "\n",
    "for n, token_list in enumerate(splittet_train_trf):\n",
    "    for i in tqdm(range(len(token_list))):\n",
    "        train_noun_text_splits[n] = train_noun_text_splits[n] + [token.text for token in token_list[i] \n",
    "                                                               if(not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\")]\n",
    "        \n",
    "train_nouns_text = train_noun_text_splits[0]\n",
    "\n",
    "for i in range(1,splits):\n",
    "    train_nouns_text = train_nouns_text + train_noun_text_splits[i]\n",
    "\n",
    "with open('train_nouns_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_nouns_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 8\n",
    "train_noun_lemma_splits = {}\n",
    "for i in range(splits):\n",
    "    train_noun_lemma_splits[i] = []\n",
    "    \n",
    "splittet_train_trf = np.array_split(train_doc_trf_loaded, splits)\n",
    "\n",
    "for n, token_list in enumerate(splittet_train_trf):\n",
    "    for i in tqdm(range(len(token_list))):\n",
    "        train_noun_lemma_splits[n] = train_noun_lemma_splits[n] + [token.lemma_ for token in token_list[i] \n",
    "                                                               if(not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\")]\n",
    "        \n",
    "train_nouns_lemma = train_noun_lemma_splits[0]\n",
    "\n",
    "for i in range(1,splits):\n",
    "    train_nouns_lemma = train_nouns_lemma + train_noun_lemma_splits[i]\n",
    "\n",
    "with open('train_nouns_lemma.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_nouns_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Datasets with Spacy (old way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete_text = \"\"\n",
    "val_complete_text = \"\"\n",
    "test_complete_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text_dataset.shape[0])\n",
    "print(val_text_dataset.shape[0])\n",
    "print(test_text_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Datasets to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(train_text_dataset.shape[0])):\n",
    "    train_complete_text = train_complete_text + train_text_dataset[i]['sentence']\n",
    "\n",
    "for i in tqdm(range(val_text_dataset.shape[0])):\n",
    "    val_complete_text = val_complete_text + val_text_dataset[i]['sentence']\n",
    "\n",
    "for i in tqdm(range(test_text_dataset.shape[0])):\n",
    "    test_complete_text = test_complete_text + test_text_dataset[i]['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Datasets with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"start process val @ \", now)\n",
    "\n",
    "val_doc = nlp(val_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process val @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print(\"start process test @ \", now)\n",
    "\n",
    "test_doc_trf = nlp(test_complete_text, disable = ['ner', 'parser'])\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"End process test @ \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tokens that arent stop words or punctuations\n",
    "train_all = [token.text\n",
    "         for token in train_doc\n",
    "         if not token.is_punct]\n",
    "\n",
    "# all tokens that arent stop words or punctuations\n",
    "train_words = [token.text\n",
    "         for token in train_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "train_nouns = [token.text\n",
    "         for token in train_doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun tokens that arent stop words or punctuations\n",
    "train_nouns_lemma = [token.lemma_\n",
    "         for token in train_doc_trf_loaded\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results check WER (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris/checkpoint-51000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    input_values = processor(\n",
    "          batch[\"speech\"], \n",
    "          sampling_rate=batch[\"sampling_rate\"], \n",
    "          return_tensors=\"pt\"\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "test_sampled = load_from_disk(\"/media/chris/TheFlash/Master/data/test_sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_sampled.map(map_to_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.save_to_disk(\"results51_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_complete_text = \"\"\n",
    "for i in tqdm(range(results.shape[0])):\n",
    "    result_complete_text = result_complete_text + results[i]['pred_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc_trf = nlp(result_complete_text, disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc_trf = []\n",
    "for i in tqdm(range(results.shape[0])):\n",
    "    result_doc_trf.append(nlp(results[i]['pred_str'], disable = ['ner', 'parser']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_doc_trf.pickle', 'wb') as handle:\n",
    "    pickle.dump(result_doc_trf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
