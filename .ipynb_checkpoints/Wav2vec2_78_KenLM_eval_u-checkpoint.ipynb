{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kenlm\n",
    "import ctcdecode\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris-cut/checkpoint-51000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = tokenizer.get_vocab()\n",
    "sort_vocab = sorted((value, key) for (key,value) in vocab_dict.items())\n",
    "vocab = [x[1].replace(\"|\", \" \") if x[1] not in tokenizer.all_special_tokens else \"_\" for x in sort_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1gram\n",
      "found 2gram\n"
     ]
    }
   ],
   "source": [
    "alpha = 2.5 # LM Weight\n",
    "beta = 0.0 # LM Usage Reward\n",
    "word_lm_scorer = ctcdecode.WordKenLMScorer('train.arpa', alpha, beta) # use your own kenlm model\n",
    "\n",
    "decoder = ctcdecode.BeamSearchDecoder(\n",
    "    vocab,\n",
    "    num_workers=4,\n",
    "    beam_width=128,\n",
    "    scorers=[word_lm_scorer],\n",
    "    cutoff_prob=np.log(0.000001),\n",
    "    cutoff_top_n=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results51 = load_from_disk(\"res51_full_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled = load_from_disk(\"/media/chris/TheFlash/Master/data/test_sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_index = []\n",
    "for i in range(200):\n",
    "    select_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results51_small = results51.select(select_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results51_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_lm(batch, word_lm_scorer, vocabulary):\n",
    "    from ctcdecode.prefix import State\n",
    "    import numpy as np\n",
    "\n",
    "    def get_pruned_vocab_indices(log_probs):\n",
    "        \"\"\" Return vocab indices of pruned probabilities of a time step. \"\"\"\n",
    "\n",
    "        index_to_prob = [(k, log_probs[k]) for k in range(log_probs.shape[0])]\n",
    "        index_to_prob = sorted(index_to_prob, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if 40 < len(index_to_prob):\n",
    "            index_to_prob = index_to_prob[:40]\n",
    "\n",
    "        if np.log(0.000001) < 1.0:\n",
    "            filtered = []\n",
    "            for x in index_to_prob:\n",
    "                if x[1] >= np.log(0.000001):\n",
    "                    filtered.append(x)\n",
    "            index_to_prob = filtered\n",
    "\n",
    "        return [x[0] for x in index_to_prob]\n",
    "\n",
    "    def decode(probs):\n",
    "        # Num time steps\n",
    "        nT = probs.shape[0]\n",
    "\n",
    "        # Initialize prefixes\n",
    "        prefixes = State(\n",
    "            scorers=[word_lm_scorer],\n",
    "            size=128\n",
    "        )\n",
    "\n",
    "        # Iterate over timesteps\n",
    "        for t in range(nT):\n",
    "            step_probs = probs[t]\n",
    "            pruned_step_probs = get_pruned_vocab_indices(step_probs)\n",
    "\n",
    "            # Iterate over symbols\n",
    "            for v in pruned_step_probs:\n",
    "                symbol = vocabulary[v]\n",
    "                symbol_prob = step_probs[v]\n",
    "\n",
    "                # Iterate over prefixes\n",
    "                for prefix in prefixes:\n",
    "\n",
    "                    # If there is a blank, we extend the existing prefix\n",
    "                    if symbol == '_':\n",
    "                        prefix.add_p_blank(symbol_prob + prefix.score)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # If the last symbol is repeated\n",
    "                        # update the existing prefix\n",
    "                        if symbol == prefix.symbol:\n",
    "                            p = symbol_prob + prefix.p_non_blank_prev\n",
    "                            prefix.add_p_non_blank(p)\n",
    "\n",
    "                        new_prefix = prefixes.get_prefix(prefix, symbol)\n",
    "\n",
    "                        if new_prefix is not None:\n",
    "                            p = -np.inf\n",
    "\n",
    "                            if symbol == prefix.symbol and \\\n",
    "                                    prefix.p_blank_prev > -np.inf:\n",
    "                                p = prefix.p_blank_prev + symbol_prob\n",
    "\n",
    "                            elif prefix.symbol != symbol:\n",
    "                                p = prefix.score + symbol_prob\n",
    "\n",
    "                            new_prefix.add_p_non_blank(p)\n",
    "\n",
    "            prefixes.step()\n",
    "\n",
    "        prefixes.finalize()\n",
    "\n",
    "        return prefixes.best()\n",
    "\n",
    "    batch[\"lm_str\"] = decode(np.asarray(batch[\"lm_raw\"]))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412.7289090156555\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "results51_small = results51_small.map(infer_lm, fn_kwargs=dict(word_lm_scorer=word_lm_scorer, vocabulary=vocab), num_proc=4)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15588\n"
     ]
    }
   ],
   "source": [
    "print(len(results51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "batch_size = 2000\n",
    "dataset_length = len(results51)\n",
    "\n",
    "for i in range(0, dataset_length, batch_size):\n",
    "    chunk = Dataset.from_dict(results51[i:i+batch_size])\n",
    "    chunk = chunk.map(infer_lm, fn_kwargs=dict(word_lm_scorer=word_lm_scorer, vocabulary=vocab), num_proc=4)\n",
    "    chunk.save_to_disk(\"lm_data/chunk\" + str(i) + \"_\" + str(i+batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "cv_sampled_test = load_from_disk(\"lm_data/chunk0_2000\")\n",
    "\n",
    "for file_name in glob.iglob(\"cv_sampled/chunk*\"):\n",
    "    if(file_name ==\"cv_sampled/data_0_5000\"):\n",
    "        i= 0\n",
    "        # do nothing\n",
    "    else:\n",
    "        print(file_name)\n",
    "        cv_batch = load_from_disk(file_name)\n",
    "        cv_sampled_test = concatenate_datasets([cv_sampled_test, cv_batch])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lm_str</th>\n",
       "      <th>pred_str</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diese aneignung vollzieht sich vielfach auf der grundlage eingef√ºhrte esoterischer vorstellungen</td>\n",
       "      <td>diese aneignung vollzieht sieh vielfach auf der grundlage eingef√ºhrter esotherischer vorstellungen</td>\n",
       "      <td>diese aneignung vollzieht sich vielfach auf der grundlage eingef√ºhrter esoterischer vorstellungen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>au√üerdem gibt es ein r√∂hrenwerk und betriebe der bauwirtschaft</td>\n",
       "      <td>au√üerdem gibt es ein r√∂hrenwerk und betriebe der bauwirtschaft</td>\n",
       "      <td>au√üerdem gibt es ein r√∂hrenwerk und betriebe der bauwirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>der empf√§nger erh√§lt also ein m√∂glicherweise ver√§ndertes wort</td>\n",
       "      <td>der empf√§nger erh√§lt also ein m√∂glicherweise ver√§ndertes wort</td>\n",
       "      <td>der empf√§nger erh√§lt also ein m√∂glicherweise ver√§ndertes wort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trank im laven</td>\n",
       "      <td>trankt im naven</td>\n",
       "      <td>rang im norden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>die reaktion von wei√üem phosphor mit sauerstoff ist stark exotherm</td>\n",
       "      <td>die reaktion von wei√üem phosphuar mit sauerstoff ist stark exotern</td>\n",
       "      <td>die reaktion von wei√üem phosphor mit sauerstoff ist stark exotherm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>der datenschutz ist gew√§hrleistet</td>\n",
       "      <td>der datenschutz ist gew√§hrleistet</td>\n",
       "      <td>der datenschutz ist gew√§hrleistet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sinus folge leisten</td>\n",
       "      <td>simus folgeleistenst</td>\n",
       "      <td>sie muss folge leisten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>platz besitzt einen bahnhof wanten bahnstrecken rostock neustrelitz und platz g√ºstrow</td>\n",
       "      <td>plaz besitzt einen bahnhof wantenwahnstreckenrostock neustrelitz und plazg√ºstrow</td>\n",
       "      <td>plaaz besitzt einen bahnhof an den bahnstrecken rostockneustrelitz und plaazg√ºstrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>im ort gibt es zwei kirchen und ein herrenhaus</td>\n",
       "      <td>im ort gibt es zwei kirchen und ein herrenhaus</td>\n",
       "      <td>im ort gibt es zwei kirchen und ein herrenhaus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>die unabh√§ngige schiri kann sich dabei auch gegen die vergabe eines pr√§dikats entscheiden</td>\n",
       "      <td>die unabh√§ngige schwiri kann sich dabei auch gegen die vergabain ds pr√§dikats entschalden</td>\n",
       "      <td>die unabh√§ngige jury kann sich dabei auch gegen die vergabe eines pr√§dikats entscheiden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(results51_small.remove_columns([\"speech\", \"sampling_rate\", \"lm_raw\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.204\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_small[\"lm_str\"], references=results51_small[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.237\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results51_small[\"pred_str\"], references=results51_small[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
