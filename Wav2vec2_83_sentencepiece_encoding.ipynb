{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='one.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "test_sampled_pro = load_from_disk(\"E:/Master/data/test_sampled_pro\")\n",
    "#train_sampled_pro = load_from_disk(\"E:/Master/data/train_sampled_pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 16, 12, 26, 29, 7, 0, 11, 0, 21, 11, 12, 26, 23, 30, 26, 14, 29, 26, 12, 21, 23, 16, 10, 26, 14, 26, 14, 29, 10, 14, 29, 1, 0, 8, 26, 1, 19, 27, 19, 6, 10, 14, 26, 14, 29, 15, 10, 26, 29, 8, 26, 12, 29, 3, 0, 27, 18, 26, 29, 16, 19, 12, 4, 26, 12, 21, 29, 22, 19, 6, 19, 19, 12, 29, 18, 14, 8, 29, 1, 19, 12, 10, 26, 29, 23, 17, 19, 10, 12, 26, 29]\n"
     ]
    }
   ],
   "source": [
    "test_data = test_sampled_pro[2]\n",
    "print(test_data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felipe hat eine auch für monarchen ungewöhnlich lange titelliste \n"
     ]
    }
   ],
   "source": [
    "res = \"\"\n",
    "for i in range(len(test_data[\"labels\"])):\n",
    "    res = res + (labels[test_data[\"labels\"][i]])\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felipe hat eine auch für monarchen ungewöhnlich lange titelliste \n"
     ]
    }
   ],
   "source": [
    "res_2 = ''.join([labels[i] for i in test_data[\"labels\"]])\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_old = {\"o\": 0, \"m\": 1, \"ö\": 2, \"v\": 3, \"p\": 4, \"y\": 5, \"z\": 6, \"f\": 7, \"d\": 8, \"j\": 9, \"i\": 10, \"t\": 11, \"r\": 12, \"ä\": 13, \"n\": 14, \"w\": 15, \"h\": 16, \"l\": 17, \"u\": 18, \"a\": 19, \"x\": 20, \"s\": 21, \"b\": 22, \"c\": 23, \"ß\": 24, \"ü\": 25, \"e\": 26, \"g\": 27, \"q\": 28, \"k\": 30, \"|\": 29, \"[UNK]\": 31, \"[PAD]\": 32}\n",
    "\n",
    "vocab_old[\"-\"] = len(vocab_old)\n",
    "\n",
    "import json\n",
    "with open('vocab_new.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_old, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁fe', 'lip', 'e', '▁hat', '▁eine', '▁auch', '▁für', '▁monarchen', '▁ungewöhnlich', '▁lange', '▁titel', 'liste']\n",
      "-fe lip e -hat -eine -auch -für -monarchen -ungewöhnlich -lange -titel liste\n"
     ]
    }
   ],
   "source": [
    "encoded = s.encode(res, out_type=str)\n",
    "tokenized = ' '.join(encoded)\n",
    "tokenized = tokenized.replace(\"▁\",\"-\")\n",
    "print(encoded)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 7, 26, 29, 17, 10, 4, 29, 26, 29, 33, 16, 19, 11, 29, 33, 26, 10, 14, 26, 29, 33, 19, 18, 23, 16, 29, 33, 7, 25, 12, 29, 33, 1, 0, 14, 19, 12, 23, 16, 26, 14, 29, 33, 18, 14, 27, 26, 15, 2, 16, 14, 17, 10, 23, 16, 29, 33, 17, 19, 14, 27, 26, 29, 33, 11, 10, 11, 26, 17, 29, 17, 10, 21, 11, 26]\n"
     ]
    }
   ],
   "source": [
    "labels_new = []\n",
    "for i in range(len(tokenized)):\n",
    "    if tokenized[i] == \" \":\n",
    "        labels_new.append(vocab_old[\"|\"])\n",
    "    else:\n",
    "        labels_new.append(vocab_old[tokenized[i]])\n",
    "\n",
    "print(labels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-fe lip e -hat -eine -auch -für -monarchen -ungewöhnlich -lange -titel liste\n"
     ]
    }
   ],
   "source": [
    "labels_inv = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_', '-']\n",
    "\n",
    "res_back = \"\"\n",
    "for i in range(len(labels_new)):\n",
    "    res_back = res_back + (labels_inv[labels_new[i]])\n",
    "    \n",
    "print(res_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁fe', 'lip', 'e', '▁hat', '▁eine', '▁auch', '▁für', '▁monarchen', '▁ungewöhnlich', '▁lange', '▁titel', 'liste']\n"
     ]
    }
   ],
   "source": [
    "res_back = res_back.replace(\"-\",\"▁\")\n",
    "res_back = res_back.split(\" \")\n",
    "print(res_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felipe hat eine auch für monarchen ungewöhnlich lange titelliste\n"
     ]
    }
   ],
   "source": [
    "detokenized = s.decode(res_back)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felipe hat eine auch für monarchen ungewöhnlich lange titelliste\n"
     ]
    }
   ],
   "source": [
    "detokenized = s.decode(encoded)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_']\n",
    "vocab_old = {\"o\": 0, \"m\": 1, \"ö\": 2, \"v\": 3, \"p\": 4, \"y\": 5, \"z\": 6, \"f\": 7, \"d\": 8, \"j\": 9, \"i\": 10, \"t\": 11, \"r\": 12, \"ä\": 13, \"n\": 14, \"w\": 15, \"h\": 16, \"l\": 17, \"u\": 18, \"a\": 19, \"x\": 20, \"s\": 21, \"b\": 22, \"c\": 23, \"ß\": 24, \"ü\": 25, \"e\": 26, \"g\": 27, \"q\": 28, \"k\": 30, \"|\": 29, \"[UNK]\": 31, \"[PAD]\": 32}\n",
    "vocab_old[\"-\"] = len(vocab_old)\n",
    "labels_inv = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentencepiece(batch):\n",
    "    \n",
    "    import sentencepiece as spm\n",
    "    s = spm.SentencePieceProcessor(model_file='one.model')\n",
    "    \n",
    "    labels = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_']\n",
    "    vocab_old = {\"o\": 0, \"m\": 1, \"ö\": 2, \"v\": 3, \"p\": 4, \"y\": 5, \"z\": 6, \"f\": 7, \"d\": 8, \"j\": 9, \"i\": 10, \"t\": 11, \"r\": 12, \"ä\": 13, \"n\": 14, \"w\": 15, \"h\": 16, \"l\": 17, \"u\": 18, \"a\": 19, \"x\": 20, \"s\": 21, \"b\": 22, \"c\": 23, \"ß\": 24, \"ü\": 25, \"e\": 26, \"g\": 27, \"q\": 28, \"k\": 30, \"|\": 29, \"[UNK]\": 31, \"[PAD]\": 32}\n",
    "    vocab_old[\"-\"] = len(vocab_old)\n",
    "    labels_inv = ['o', 'm', 'ö', 'v', 'p', 'y', 'z', 'f', 'd', 'j', 'i', 't', 'r', 'ä', 'n', 'w', 'h', 'l', 'u', 'a', 'x', 's', 'b', 'c', 'ß', 'ü', 'e', 'g', 'q', ' ', 'k', '_', '_', '-']\n",
    "\n",
    "    batch[\"encoded\"] = s.encode(''.join([labels[i] for i in batch[\"labels\"]]), out_type=str)\n",
    "    batch[\"tokenized\"] = ' '.join(batch[\"encoded\"] ).replace(\"▁\",\"-\").replace(\" \",\"|\")  \n",
    "    batch[\"tokenized\"] = batch[\"tokenized\"] + \"|\"\n",
    "    batch[\"labels_new\"] = [vocab_old[i] for i in batch[\"tokenized\"]]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled_pro_new = test_sampled_pro.map(add_sentencepiece, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(test_sampled_pro[i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test_sampled_pro_new[i][\"labels_new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled_pro_new.save_to_disk(\"E:/Master/data/test_sampled_pro_piece\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-ihre|-foto|strecken|-erschienen|-in|-mode|magazin|en|-wie|-der|-vogue|-har|per|s|-ba|za|ar|-und|-marie|-claire\n"
     ]
    }
   ],
   "source": [
    "encoded_test = test_sampled_pro[2][\"encoded\"]\n",
    "tokenized_test  = ' '.join(encoded_test).replace(\"▁\",\"-\").replace(\" \",\"|\")\n",
    "print(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 10, 16, 12, 26, 29, 33, 7, 0, 11, 0, 29, 21, 11, 12, 26, 23, 30, 26, 14, 29, 33, 26, 12, 21, 23, 16, 10, 26, 14, 26, 14, 29, 33, 10, 14, 29, 33, 1, 0, 8, 26, 29, 1, 19, 27, 19, 6, 10, 14, 29, 26, 14, 29, 33, 15, 10, 26, 29, 33, 8, 26, 12, 29, 33, 3, 0, 27, 18, 26, 29, 33, 16, 19, 12, 29, 4, 26, 12, 29, 21, 29, 33, 22, 19, 29, 6, 19, 29, 19, 12, 29, 33, 18, 14, 8, 29, 33, 1, 19, 12, 10, 26, 29, 33, 23, 17, 19, 10, 12, 26]\n"
     ]
    }
   ],
   "source": [
    "labels_new_test = [vocab_old[i] for i in tokenized_test]\n",
    "print(labels_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_raw_cut_2.save_to_disk(\"E:/Master/data/0_text/train_text_raw_cut_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
