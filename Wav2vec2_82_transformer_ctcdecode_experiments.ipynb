{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kenlm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, load_metric, Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pred_data = load_from_disk(\"lm_res51/res51_full_log_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = raw_pred_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "d:\\transformers\\src\\transformers\\models\\auto\\modeling_auto.py:829: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, \n",
    "    AutoTokenizer, AutoModelWithLMHead\n",
    ")\n",
    "device = \"cuda\"    \n",
    "\n",
    "wav_tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "wav_feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "wav_processor = Wav2Vec2Processor(feature_extractor=wav_feature_extractor, tokenizer=wav_tokenizer)\n",
    "\n",
    "wav_model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-large-xlsr-ger-chris/checkpoint-51000\").to(device)\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "\n",
    "gpt_model = AutoModelWithLMHead.from_pretrained(\"dbmdz/german-gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ihre fortestrecken erschienen mit molemagazine wie der wog at das basarwaryclar\n",
      "torch.Size([396, 33])\n",
      "tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 10, 32,\n",
      "        16, 32, 32, 12, 26, 26, 32, 29, 32, 32,  7, 32, 32,  0, 32, 12, 32, 32,\n",
      "        11, 32, 32, 26, 32, 32, 32, 32, 21, 32, 11, 12, 32, 26, 32, 23, 32, 32,\n",
      "        32, 32, 30, 26, 32, 32, 14, 29, 29, 32, 32, 26, 32, 12, 32, 21, 21, 23,\n",
      "        23, 16, 32, 10, 32, 26, 26, 32, 14, 14, 32, 26, 14, 32, 29, 29,  1, 10,\n",
      "        32, 32, 32, 11, 32, 29, 32,  1, 32,  0, 32, 32, 32, 32, 32, 17, 26, 32,\n",
      "        32, 32, 32,  1, 32, 19, 32, 32, 32, 32, 27, 32, 32, 19, 32, 32, 32, 32,\n",
      "        32, 32,  6, 32, 32, 10, 32, 32, 32, 32, 14, 14, 26, 32, 29, 32, 15, 32,\n",
      "        10, 26, 32, 29,  8, 32, 26, 32, 12, 29, 29, 32, 15, 32, 32, 32, 32,  0,\n",
      "        32, 32, 32, 32, 32, 32, 27, 32, 32, 32, 29, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 19, 32, 32, 32, 11, 32, 29, 32,  8, 32, 19, 32, 32, 21, 32,\n",
      "        32, 29, 32, 22, 32, 32, 19, 32, 32, 32, 32, 32, 21, 32, 19, 32, 32, 32,\n",
      "        32, 32, 12, 32, 32, 32, 32, 32, 32, 15, 32, 32, 19, 32, 32, 32, 32, 12,\n",
      "        32, 32,  5, 32, 32, 32, 32, 32, 32, 32, 23, 32, 32, 17, 32, 19, 32, 32,\n",
      "        32, 32, 12, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 29],\n",
      "       device='cuda:0')\n",
      "33\n",
      "torch.Size([393, 33])\n",
      "tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 10,\n",
      "        32, 16, 32, 32, 12, 26, 26, 32, 29, 32, 32,  7, 32, 32, 32, 12, 32, 32,\n",
      "        11, 32, 32, 26, 32, 32, 32, 32, 21, 32, 11, 12, 32, 26, 32, 23, 32, 32,\n",
      "        32, 32, 30, 26, 32, 32, 14, 29, 29, 32, 32, 26, 32, 12, 32, 21, 21, 23,\n",
      "        23, 16, 32, 10, 32, 26, 26, 32, 14, 14, 32, 26, 14, 32, 29, 29,  1, 10,\n",
      "        32, 32, 32, 11, 32, 29, 32,  1, 32, 32, 32, 32, 32, 32, 17, 26, 32, 32,\n",
      "        32, 32,  1, 32, 19, 32, 32, 32, 32, 27, 32, 32, 19, 32, 32, 32, 32, 32,\n",
      "        32,  6, 32, 32, 10, 32, 32, 32, 32, 14, 14, 26, 32, 29, 32, 15, 32, 10,\n",
      "        26, 32, 29,  8, 32, 26, 32, 12, 29, 29, 32, 15, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 27, 32, 32, 32, 29, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 19, 32, 32, 32, 11, 32, 29, 32,  8, 32, 19, 32, 32, 21, 32, 32, 29,\n",
      "        32, 22, 32, 32, 19, 32, 32, 32, 32, 32, 21, 32, 19, 32, 32, 32, 32, 32,\n",
      "        12, 32, 32, 32, 32, 32, 32, 15, 32, 32, 19, 32, 32, 32, 32, 12, 32, 32,\n",
      "         5, 32, 32, 32, 32, 32, 32, 32, 23, 32, 32, 17, 32, 19, 32, 32, 32, 32,\n",
      "        12, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 29],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (52000) must match the size of tensor b (33) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d42a663fd181>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mgpt_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpt_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpt_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvoice_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mcomb_pred_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpt_prob\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvoice_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mdecoded_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomb_pred_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (52000) must match the size of tensor b (33) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "features = wav_processor(\n",
    "      test_data[\"speech\"], \n",
    "      sampling_rate=test_data[\"sampling_rate\"], \n",
    "      padding=True,\n",
    "      return_tensors=\"pt\")\n",
    "\n",
    "input_values = features.input_values.to(\"cuda\")\n",
    "\n",
    "attention_mask = features.attention_mask.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = wav_model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    test_data[\"pred_str\"] = wav_processor.batch_decode(pred_ids)[0]\n",
    "    \n",
    "    print(test_data[\"pred_str\"])\n",
    "\n",
    "    decoded_results = []\n",
    "    for logit in logits:\n",
    "        print(logit.shape)\n",
    "        pred_ids = torch.argmax(logit, dim=-1)\n",
    "        print(pred_ids)\n",
    "        mask = pred_ids.ge(1).unsqueeze(-1).expand(logit.size())\n",
    "        vocab_size = logit.size()[-1]\n",
    "        print(vocab_size)\n",
    "        voice_prob = torch.nn.functional.softmax((torch.masked_select(logit, mask).view(-1,vocab_size)),dim=-1)\n",
    "        print(voice_prob.shape)\n",
    "        \n",
    "        gpt_input = torch.cat((torch.tensor([32]).to(device),pred_ids[pred_ids>0]), 0)\n",
    "        print(gpt_input)\n",
    "        gpt_prob = torch.nn.functional.softmax(gpt_model(gpt_input).logits, dim=-1)[:voice_prob.size()[0],:]\n",
    "       \n",
    "        comb_pred_ids = torch.argmax(gpt_prob*voice_prob, dim=-1)\n",
    "        decoded_results.append(wav_processor.decode(comb_pred_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127104])\n"
     ]
    }
   ],
   "source": [
    "labels = input_values.masked_fill(attention_mask.ne(1), -100)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ihre fotostrecken erschienen in modemagazinen wie der vogue harpers bazaar und marie claire \n"
     ]
    }
   ],
   "source": [
    "print(test_data[\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:52000 for open-end generation.\n",
      "Input length of input_ids is 22, but ``max_length`` is set to 12.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "[{'generated_text': 'ihre fortestrecken erschienen mit molemagazine wie der wog at das basarwaryclar,'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text-generation', model=\"dbmdz/german-gpt2\",\n",
    "                 tokenizer=\"dbmdz/german-gpt2\")\n",
    "\n",
    "features = wav_processor(\n",
    "      test_data[\"speech\"], \n",
    "      sampling_rate=test_data[\"sampling_rate\"], \n",
    "      padding=True,\n",
    "      return_tensors=\"pt\")\n",
    "\n",
    "input_values = features.input_values.to(\"cuda\")\n",
    "\n",
    "attention_mask = features.attention_mask.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = wav_model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    test_data[\"pred_str\"] = wav_processor.batch_decode(pred_ids)[0]\n",
    "    \n",
    "    print(len(test_data[\"pred_str\"]))\n",
    "    \n",
    "    out_text = pipe(test_data[\"pred_str\"])\n",
    "    print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
